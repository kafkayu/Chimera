{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99970ac-31d4-4165-868a-03dd86936d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 00:23:41.630957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/var/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:106: UserWarning: \n",
      "\n",
      "================================================================================\n",
      "WARNING: Manual override via BNB_CUDA_VERSION env variable detected!\n",
      "BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "Loading CUDA version: BNB_CUDA_VERSION=122\n",
      "================================================================================\n",
      "\n",
      "\n",
      "  warn((f'\\n\\n{\"=\"*80}\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-10 00:23:43,461] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import Trainer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1038d11-ef11-40ec-a0dc-a8a330667c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"../../../../../model/vicuna-7b-v1.3\"\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0169b72-2a5a-46c0-ae74-b6c00406917f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a607a516a6f4469097e8803b77986a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config if load_in_4bit else None,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5687be-476e-44f3-b0e1-9be090d06fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_max_length=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b2dc86-4863-4ef9-88c8-6b83080d8066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        model_max_length=model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94f65ca4-dad6-49c3-9b45-5a9d4d87c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\"\n",
    "with open(filename, 'r') as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6c2c071c-84c5-4b69-acee-5b7a8c8c08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {'question':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "078dd61e-0516-428c-ad2d-176f592d0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from': 'human', 'value': 'What is the goal of dating?'}\n",
      "{'from': 'human', 'value': 'and add condition styling if time more than \"9:00\"'}\n",
      "{'from': 'human', 'value': 'Can you write me a homily on a particular subject?'}\n",
      "{'from': 'human', 'value': 'أكتبلي سكريبت اعلان لشركة هواتف محمولة لهاتف للفئة المتوسطة موجه للشباب مدمنى العاب الفيديو جيمز'}\n",
      "{'from': 'human', 'value': 'let\\'s take Andy Clark\\'s view here from his book \"natural born cyborgs\"'}\n",
      "{'from': 'human', 'value': 'I want to design an event that signify the concept of The Force, and create a fun, light-hearted, positive, imaginative feeling. Which movie/event is the best to based on then?'}\n",
      "{'from': 'human', 'value': 'Explain in detail \"What are the key factors that determine the success or failure of organizational change initiatives?\"'}\n",
      "{'from': 'human', 'value': 'Steps to configure Azure server to upload a site that includes Front and Back end'}\n",
      "{'from': 'human', 'value': \"ok so i need to plan for tomorrow. i'll give you my schedule\"}\n",
      "{'from': 'human', 'value': \"give 3 more simple versions and just one interrogative and opposite meaning at the end : he's having a crisis at work.\"}\n",
      "{'from': 'human', 'value': 'scniff spoofed packets in scapy'}\n",
      "{'from': 'human', 'value': 'Write me a Typescript function to tell if a number between 0 and 100 is even or odd, in the least optimal way possible and using switch statements'}\n",
      "{'from': 'human', 'value': \"I have problem with this code: fixed (byte\\\\* data = &hashData[0]) it doesn't work when hashData array has no elements. Why, and how to fix it?\"}\n"
     ]
    }
   ],
   "source": [
    "for i in train:\n",
    "    for j  in i['conversations']:\n",
    "        if j['from'] =='human':\n",
    "            print(j)\n",
    "            Q['question'].append(j)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f6969cd5-b3df-497f-9562-ca1e684d23b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( \"../../../../../data/ShareGPT_Vicuna_unfiltered/small_question.json\", \"w\") as json_file:\n",
    "    json.dump(Q, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "048d39d3-b59a-484a-84c9-48d951fb73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( \"../../../../../data/ShareGPT_Vicuna_unfiltered/small_question.json\", \"r\") as json_file:\n",
    "   Qs =  json.load( json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6beb7aa9-9f92-49dd-a8e1-f68e68dfc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =  \"../../../../../data/ShareGPT_Vicuna_unfiltered/train_question.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "87aa2a9a-f0a0-477d-913f-d6da14947791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已经保存0个question\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Q['question'])):\n",
    "    if i % 500 == 0:\n",
    "        with open( filename, \"w\") as json_file:\n",
    "            json.dump(Q,json_file)\n",
    "        print(\"已经保存{}个question\".format(i))\n",
    "    input = tokenizer([Q['question'][i]['value']])\n",
    "    output = model.generate(inputs = torch.tensor(input['input_ids'] ),max_length = len(input['input_ids'][0])*8,early_stopping =True)\n",
    "    text = tokenizer.decode(output[0],skip_special_tokens =True)\n",
    "    ques['question'][i]['answer'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f79d0-af5f-4999-93a7-a12f661e9686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
