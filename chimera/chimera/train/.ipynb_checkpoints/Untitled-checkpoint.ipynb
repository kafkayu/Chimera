{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b9422f-cb7c-40ec-a4f3-143a0e0f6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is based on tatsu-lab/stanford_alpaca. Below is the original copyright:\n",
    "#\n",
    "#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "\n",
    "# Adapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer, BitsAndBytesConfig\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "\n",
    "from fastchat.conversation import SeparatorStyle\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "from torch.nn import CrossEntropyLoss,MSELoss\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "from medusa.model.medusa_model import MedusaModel, MedusaConfig,SingleMedusa\n",
    "import torch.nn.functional as F\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "\n",
    "\n",
    "# Customized for training Medusa heads\n",
    "class CustomizedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # DDP will give us model.module\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if hasattr(model, \"module\"):\n",
    "            medusa = model.module.medusa\n",
    "        else:\n",
    "            medusa = model.medusa\n",
    "\n",
    "        logits1 = model(\n",
    "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        logits =logits1['logtis']\n",
    "        \n",
    "        labels = inputs[\"labels\"]\n",
    "        # Shift so that tokens < n predict n\n",
    "        loss = 0\n",
    "        loss_fct =CrossEntropyLoss()\n",
    "        log = {}\n",
    "        #logits = torch.clamp(logits, min=1e-7, max=100 - 1e-7)\n",
    "        for i in range(medusa):\n",
    "            #########修改后#######\n",
    "            # medusa_logits = logits[i, :, : -1].contiguous()\n",
    "            \n",
    "            # medusa_labels = labels[...,  2:].contiguous()\n",
    "            ######原medusa#########\n",
    "            \n",
    "            medusa_logits = logits[i, :, : ].contiguous()\n",
    "            \n",
    "            medusa_labels = labels[...,  3:].contiguous()\n",
    "            medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "            medusa_labels = medusa_labels.view(-1)\n",
    "            \n",
    "            medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "            \n",
    "            #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=100 - 1e-7)\n",
    "           \n",
    "            loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "            loss += loss_i\n",
    "            not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "            medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "            # Add top-k accuracy\n",
    "            for k in range(1, 6):\n",
    "                _, topk = medusa_logits.topk(k, dim=-1)\n",
    "                topk = topk[not_ignore]\n",
    "                correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "                log[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "        \n",
    "            \n",
    "            log[f\"medusa{i}_loss\"] = loss_i.item()\n",
    "            #log[f\"medusa{i}_loss_7\"] = loss_i_7.item()\n",
    "        self.log(log)\n",
    "        if model.training == False :\n",
    "            logeval  = {}\n",
    "            for k in range(1, 6):\n",
    "                logeval[f\"eval_medusa{0}_top{k}\"] = log[f\"medusa{0}_top{k}\"]\n",
    "            logeval[f\"eval_medusa{0}_loss\"] = log[f\"medusa{0}_loss\"]\n",
    "            self.log(logeval)\n",
    "        return (loss, logits) if return_outputs else loss\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"lmsys/vicuna-7b-v1.3\")\n",
    "    load_in_4bit: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Load in 4 bit.\"},\n",
    "    )\n",
    "    load_in_8bit: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Load in 8 bit.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(\n",
    "        default=\"sharegpt_clean.json\",\n",
    "        metadata={\"help\": \"Path to the training data.\"},\n",
    "    )\n",
    "    eval_data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n",
    "    )\n",
    "    lazy_preprocess: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=2048,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    medusa_num_heads: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Number of Medusa heads.\"},\n",
    "    )\n",
    "    medusa_num_layers: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Number of layers for each Medusa head.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "local_rank = None\n",
    "\n",
    "\n",
    "def rank0_print(*args):\n",
    "    if local_rank == 0:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conv = get_conversation_template(\"vicuna\")\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}, {j}, {role}, {conv.roles[j % 2]}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "    input_ids = tokenizer(\n",
    "        conversations,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    targets = input_ids.clone()\n",
    "\n",
    "    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n",
    "\n",
    "    # Mask targets. Only compute loss on the assistant outputs.\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        turns = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_TOKEN_ID\n",
    "        for i, turn in enumerate(turns):\n",
    "            if turn == \"\":\n",
    "                break\n",
    "            turn_len = len(tokenizer(turn).input_ids)\n",
    "\n",
    "            parts = turn.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            # \"-2\" is hardcoded for the LLaMA tokenizer to make the offset correct.\n",
    "            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            # Ignore the user instructions\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n",
    "            cur_len += turn_len\n",
    "\n",
    "        target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "        if False:  # Inspect and check the correctness of masking\n",
    "            z = target.clone()\n",
    "            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
    "            rank0_print(tokenizer.decode(z))\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_TOKEN_ID\n",
    "                rank0_print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "                    f\" (ignored)\"\n",
    "                )\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        rank0_print(\"Formatting inputs...\")\n",
    "        sources = [example[\"conversations\"] for example in raw_data]\n",
    "        data_dict = preprocess(sources, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )\n",
    "\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.raw_data = raw_data\n",
    "        self.cached_data_dict = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        if i in self.cached_data_dict:\n",
    "            return self.cached_data_dict[i]\n",
    "\n",
    "        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer)\n",
    "        ret = dict(\n",
    "            input_ids=ret[\"input_ids\"][0],\n",
    "            labels=ret[\"labels\"][0],\n",
    "            attention_mask=ret[\"attention_mask\"][0],\n",
    "        )\n",
    "        self.cached_data_dict[i] = ret\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "def make_supervised_data_module(\n",
    "    tokenizer: transformers.PreTrainedTokenizer, data_args\n",
    ") -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    dataset_cls = (\n",
    "        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n",
    "    )\n",
    "    rank0_print(\"Loading data...\")\n",
    "\n",
    "    train_json = json.load(open(data_args.data_path, \"r\"))\n",
    "    train_dataset = dataset_cls(train_json, tokenizer=tokenizer)\n",
    "\n",
    "    if data_args.eval_data_path:\n",
    "        eval_json = json.load(open(data_args.eval_data_path, \"r\"))\n",
    "        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer)\n",
    "    else:\n",
    "        eval_dataset = None\n",
    "\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad6f7e0a-5077-4a29-80b3-47bbdbe8d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "global local_rank\n",
    "import transformers \n",
    "#from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    local_rank=0,\n",
    "    model_max_length=128 ,\n",
    "    medusa_num_heads = 1 ,\n",
    "    medusa_num_layers =  1 ,\n",
    "    prediction_loss_only = False,\n",
    "    output_dir= './test', \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 1 ,\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=1e-3, \n",
    "    weight_decay=0.0,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    #fp16=True, #对应--bf16\n",
    "    #tf32=True,\n",
    "    \n",
    ")\n",
    "#from transformers import DataArguments\n",
    "\n",
    "# data_args = DataArguments(\n",
    "#     data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "#     eval_data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "#     lazy_preprocess= True \n",
    "# )\n",
    "#from transformers import ModelArguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    \n",
    "    model_name_or_path=\"../../../../../model/TinyLlama-1.1B-Chat-v0.6\",\n",
    "    #model_max_length=2048,\n",
    "    #lazy_preprocess=True,\n",
    "    # medusa_num_heads=3,\n",
    "    # medusa_num_layers=1\n",
    ")\n",
    "data_args = DataArguments(\n",
    "    data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "    eval_data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "    lazy_preprocess= True \n",
    ")\n",
    "local_rank =0 # training_args.local_rank\n",
    "\n",
    "# Set RoPE scaling factor\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    ")\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len and training_args.model_max_length > orig_ctx_len:\n",
    "    scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n",
    "    config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "config.use_cache = False\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0749e20-9e00-469c-87ff-cdb9a81a204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "        logits,labels = pred\n",
    "        #logits = pred.predictions\n",
    "        #print(logits.shape)\n",
    "        import pdb; pdb.set_trace()\n",
    "        #print(labels.shape)\n",
    "        medusa_logits = logits[0,:, : ].contiguous()\n",
    "            \n",
    "        medusa_labels = labels[...,  1:].contiguous()\n",
    "        medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "        medusa_labels = medusa_labels.view(-1)\n",
    "        \n",
    "        medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "        \n",
    "        #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=1 - 1e-7)\n",
    "       \n",
    "        loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "        loss += loss_i\n",
    "        not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "        medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "        # Add top-k accuracy\n",
    "        for k in range(1, 6):\n",
    "            _, topk = medusa_logits.topk(k, dim=-1)\n",
    "            topk = topk[not_ignore]\n",
    "            correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "            log[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "            #res[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "    \n",
    "        \n",
    "        log[f\"medusa{i}_loss\"] = loss_i.item()\n",
    "\n",
    "        \n",
    "        return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575dc631-f426-4937-bed7-75afe3bd110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../../../../model/TinyLlama-1.1B-Chat-v0.6 and are newly initialized: ['model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ../../../../../model/TinyLlama-1.1B-Chat-v0.6\n",
      "Loading data...\n",
      "Formatting inputs...Skip in lazy mode\n",
      "Formatting inputs...Skip in lazy mode\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global local_rank\n",
    "\n",
    "\n",
    "local_rank =0 #training_args.local_rank\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    ")\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len and training_args.model_max_length > orig_ctx_len:\n",
    "    scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n",
    "    config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "config.use_cache = False\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config if model_args.load_in_4bit else None,\n",
    "    load_in_4bit=model_args.load_in_4bit,\n",
    "    load_in_8bit=model_args.load_in_8bit,\n",
    ")\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "medusa_lm_head = MedusaModel(\n",
    "    model,\n",
    "    medusa_num_heads=training_args.medusa_num_heads,\n",
    "    medusa_num_layers=training_args.medusa_num_layers,\n",
    "    base_model_name_or_path=model_args.model_name_or_path\n",
    ")\n",
    "for param in medusa_lm_head.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "training_args.output_dir = f\"{training_args.output_dir}_medusa_mlp_{model_args.model_name_or_path.split('/')[-1]}_medusa_{training_args.medusa_num_heads}_lr_{training_args.learning_rate}_layers_{training_args.medusa_num_layers}\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# Load data\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "\n",
    "# Generate Medusa config for pushing to HF hub\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "# Start trainner\n",
    "trainer = CustomizedTrainer(\n",
    "    model=medusa_lm_head, tokenizer=tokenizer, args=training_args, **data_module\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2872b87-552a-411e-b41f-dfd1e0da2bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7debb2ee-d552-4878-abcc-15a1619a2470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'logeval' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/transformers/trainer.py:2654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2654\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2657\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 104\u001b[0m, in \u001b[0;36mCustomizedTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         logeval[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_medusa\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_top\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedusa\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_top\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    103\u001b[0m     logeval[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_medusa\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedusa\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[43mlogeval\u001b[49m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, logits) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'logeval' referenced before assignment"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7c2c8-e1cc-4e4b-b3b8-7368bc072f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
