{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a1af8a-2a5a-46f7-aae7-dc9fed88b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout = open('output.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f150282-017f-439f-aaec-9b976f6fa320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 12:26:06.004366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/var/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:106: UserWarning: \n",
      "\n",
      "================================================================================\n",
      "WARNING: Manual override via BNB_CUDA_VERSION env variable detected!\n",
      "BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "Loading CUDA version: BNB_CUDA_VERSION=122\n",
      "================================================================================\n",
      "\n",
      "\n",
      "  warn((f'\\n\\n{\"=\"*80}\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-11 12:26:08,204] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer, BitsAndBytesConfig\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "\n",
    "from fastchat.conversation import SeparatorStyle\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from medusa.model.medusa_model import MedusaModel, MedusaConfig,SingleMedusa\n",
    "import torch.nn.functional as F\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1e8e54-b088-4571-93d3-385479090f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timed(wall_times, key):\n",
    "    start = time.time()\n",
    "    torch.cuda.synchronize()\n",
    "    yield\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    wall_times[key].append(elapsed_time)\n",
    "\n",
    "def medusa_forward(input_ids, model, tokenizer, medusa_buffers, medusa_topk, temperature, posterior_threshold, posterior_alpha, past_key_values, past_key_values_data, current_length_data, steps = 512):\n",
    "    wall_times = {'medusa': [], 'tree': [], 'posterior': [], 'update': [], 'init': []}\n",
    "    \n",
    "    with timed(wall_times, 'init'):\n",
    "        reset_medusa_mode(model)\n",
    "        input_len = input_ids.shape[1]\n",
    "        medusa_logits, logits = initialize_medusa(input_ids, model, medusa_buffers['medusa_attn_mask'], past_key_values)\n",
    "    \n",
    "    new_token = 0\n",
    "\n",
    "    for idx in range(steps): \n",
    "        with timed(wall_times, 'medusa'):\n",
    "            candidates, tree_candidates = generate_candidates(medusa_logits, logits, medusa_topk, medusa_buffers['tree_indices'], temperature)\n",
    "\n",
    "        with timed(wall_times, 'tree'):\n",
    "            medusa_logits, logits, outputs = tree_decoding(model, tree_candidates, past_key_values, medusa_buffers['medusa_position_ids'], input_ids, medusa_buffers['retrieve_indices'])\n",
    "\n",
    "        with timed(wall_times, 'posterior'):\n",
    "            best_candidate, accept_length = evaluate_posterior(logits, candidates, temperature, posterior_threshold, posterior_alpha)\n",
    "        \n",
    "        with timed(wall_times, 'update'):\n",
    "            input_ids, logits, medusa_logits, new_token = update_inference_inputs(input_ids, candidates, best_candidate, accept_length, medusa_buffers['retrieve_indices'], outputs, logits, medusa_logits, new_token, past_key_values_data, current_length_data)\n",
    "\n",
    "        if tokenizer.eos_token_id in input_ids[0, input_len:].tolist():\n",
    "            break\n",
    "\n",
    "    return input_ids, new_token, idx, wall_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d454660b-33ba-4a49-adb4-1363dc20a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_name = '../../../../idea12_2fastlayer_0108_medusa_mlp_vicuna-7b-v1.3_medusa_1_lr_0.0001_layers_1/checkpoint-7500/pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853029aa-ef72-454d-9500-99ac31ffa154",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict =torch.load(state_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df212b09-5ab2-406b-9c13-45fc26e63d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"../../../../../model/vicuna-7b-v1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ca24f1-e131-4747-a3ae-2df3f8ed09a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b64a2f5-6d6d-493d-95b1-5097b1d3c356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c854dd978d4c6cb9e7c3f4e61504a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "        low_cpu_mem_usage=True,\n",
    " \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a21e7189-ea9b-4e43-bcea-fbe90b494318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ../../../../../model/vicuna-7b-v1.3\n"
     ]
    }
   ],
   "source": [
    "medusa_lm_head = MedusaModel(\n",
    "        model,\n",
    "        medusa_num_heads=1,\n",
    "        medusa_num_layers=1,\n",
    "        base_model_name_or_path=model_name_or_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8d1ffaa-bb18-4914-ae4f-659b88395462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medusa_lm_head.load_state_dict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a95708bd-8077-4979-8d6c-4287eea8c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_max_length=1024\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    model_max_length=model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbf8ad60-1424-4f97-98d6-ed085f7d4170",
   "metadata": {},
   "outputs": [],
   "source": [
    " def fastlayer_forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        past_key_values=None,\n",
    "        output_orig=False,\n",
    "        position_ids=None,\n",
    "        last_hs = None,\n",
    "        orig = None,\n",
    "        \n",
    "    ):\n",
    "        if last_hs is None:\n",
    "            with torch.inference_mode():\n",
    "                # Pass input through the base model\n",
    "                outputs = self.base_model.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    past_key_values=past_key_values,\n",
    "                    position_ids=position_ids,\n",
    "                    #output_hidden_states=True,\n",
    "                )\n",
    "                last_hs = outputs[0][:,:-1]\n",
    "                #orig = self.base_model.lm_head(outputs[0])\n",
    "        #####1.get trigram#####\n",
    "        embed =self.base_model.model.embed_tokens(input_ids)\n",
    "        embedtrigram = torch.cat((embed[:,:-2],embed[:,1:-1],embed[:,2:]),dim=-1)\n",
    "        gram0 = torch.cat((embed[:,0],embed[:,0],embed[:,0]),dim=-1).unsqueeze(1)\n",
    "        gram1 = torch.cat((embed[:,0],embed[:,1],embed[:,1]),dim=-1).unsqueeze(1)\n",
    "        embedtrigram = torch.cat((gram0,gram1,embedtrigram),dim=-2)\n",
    "        embedtrigram = self.trimlp(embedtrigram )\n",
    "        from modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n",
    "        batch_size, seq_length = embed.shape[:2]\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(\n",
    "                         attention_mask[:,:], (batch_size, seq_length), embed, 0\n",
    "                    )\n",
    "        attention_mask  = attention_mask.to(self.base_model.device)\n",
    "        \n",
    "        ########1.2 forward融合信息\n",
    "        for i in self.fast_layer0:\n",
    "            embedtrigram = i(embedtrigram,attention_mask =attention_mask )\n",
    "            embedtrigram = embedtrigram[0]\n",
    "        \n",
    "        # #####2.构造新的attention_mask,seq_length\n",
    "        # # ######3.构造positionid\n",
    "        position_ids = torch.arange(0, seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        #print(position_ids.shape)\n",
    "         \n",
    "        # #####4.构造新的input,计算结果\n",
    "        embed2 = torch.cat((last_hs,embedtrigram[:,-1].unsqueeze(1)),dim=-2)\n",
    "        #print(embed2.shape)\n",
    "        #print(attention_mask.shape)\n",
    "        # # # ######首先\n",
    "        attention_mask[:,:,-2,-1] = 0 \n",
    "        for i in self.fast_layer1:\n",
    "            embed2 = i(embed2 ,attention_mask = attention_mask )\n",
    "            embed2 =embed2[0]\n",
    "\n",
    "        # #######4.2和最后一层拼接进行计算\n",
    "        output2 = embed2\n",
    "        #########5.将layerN拼接作为输入预测据说效果更好 大小为seq-1，由于0，1没有trigram，实际上只有2开始有效\n",
    "        \n",
    "        #medusa_logits = []\n",
    "        #for i in range(self.medusa):\n",
    "        medusa_logits=self.medusa_head[0](embed2[:,-2] )\n",
    "        \n",
    "        return medusa_logits,embed2[:,-2]#{\"logits\":medusa_logits, dim=0,\"hs\":embed2[:,-2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77364449-3c7b-487e-a3c3-82ae68138612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictmoretoken(fastmodel,input_ids,attention_mask,outputs,k=5):\n",
    "    logit,ca = generate(fastmodel,input_ids,attention_mask,outputs,k)\n",
    "    for i in ca:\n",
    "        input_ids = torch.cat((input_ids,i.unsqueeze(0).unsqueeze(0)),dim=-1)\n",
    "        input_ids    \n",
    "    \n",
    "def calacc(input,max_length = 100,k=5):\n",
    "    input = tokenizer([inputs])\n",
    "    input_ids = torch.tensor(input.input_ids)\n",
    "    attention_mask = torch.tensor(input.attention_mask)\n",
    "    count = 0\n",
    "    for i in range(max_length):\n",
    "        \n",
    "        outputs = medusa_lm_head(input_ids ,attention_mask = attention_mask )\n",
    "        orig =  outputs['logits'][-1]\n",
    "        t0 = torch.argmax(orig[0][-1])\n",
    "        _,predictt0 =  outputs['logits'][1][0][-1].topk(5, dim=-1)\n",
    "        input_ids  = torch.cat((input_ids,t0.unsqueeze(0).unsqueeze(0)),dim=-1)\n",
    "        attention_mask = torch.cat((attention_mask,torch.tensor([[1]])),dim=-1)\n",
    "        \n",
    "        # l,ca = generate(medusa_lm_head,input_ids,attention_mask,outputs,k=k)\n",
    "        #realt1 =torch.argmax( medusa_lm_head.base_model(input_ids = input_ids)[0][0][-1])\n",
    "        \n",
    "        count+=sum(predictt0.eq(t0))\n",
    "    print(count/max_length)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10815d33-3737-460b-a152-cfea5eca11bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs =\"Please tell me a story about llama:\"# \"What is the goal of dating?:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "306b44b3-eade-4a5e-9376-e70b9ed86873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.decode(output[0])\n",
    "####generate token \n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "def headgenerate(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        past_key_values=None,\n",
    "        output_orig=False,\n",
    "        position_ids=None,\n",
    "        last_hs = None,\n",
    "        orig = None, \n",
    "        max_len = 3,\n",
    "        choices = 5,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    循环产生更多的candidate，保证无损的策略，目前是为了保证greedy search,所以需要准确预测模型的top1。\n",
    "    输入是seq_len 的last_hs,orig,输出是max_len长度的candidate序列，为了方便起见，这里直接用batch进行存储所有序列\n",
    "    运行流程：\n",
    "    1.输入orig , 预测i+1 token\n",
    "    2.拼接input_ids+ i+1\n",
    "    3.forward 获得新的fastlayer_hs和新的orig\n",
    "    4.fastlayer_hs和last_hs拼接，获得新的last_hs\n",
    "    5.拼接orig生成的newtoken到token中形成新的batch\n",
    "    \"\"\"\n",
    "    prebuffer = []\n",
    "    for i in range(max_len):\n",
    "        ####3. forward\n",
    "        orig ,fs_hs  = fastlayer_forward(medusa_lm_head,input_ids=input_ids,attention_mask=attention_mask,last_hs = last_hs)\n",
    "        #####4.拼接新last_hs\n",
    "\n",
    "        last_hs = torch.cat((last_hs,fs_hs.unsqueeze(1)),dim=-2)\n",
    "        last_hs = last_hs.repeat_interleave(choices,dim=0)\n",
    "        ####5.获取新t0\n",
    "        _,t0 = orig.topk(k=choices,dim=-1)\n",
    "        t0 = t0.unsqueeze(-1)\n",
    "        prebuffer.append(t0)\n",
    "        ######6.拼接,获得新的batch\n",
    "        input_ids =  input_ids.unsqueeze(1)\n",
    "        input_ids = input_ids.repeat_interleave(choices,dim=1)\n",
    "        input_ids = torch.cat((input_ids,t0),dim=-1)\n",
    "        #print(input_ids.shape)\n",
    "        input_ids = input_ids.flatten(0,1)\n",
    "        ######7.更新att_m\n",
    "        #import pdb;pdb.set_trace();\n",
    "        attention_mask = torch.cat((attention_mask,attention_mask[:,0].unsqueeze(1)),dim=-1)\n",
    "        attention_mask = attention_mask.repeat(choices,1)\n",
    "    return input_ids,prebuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43ef5d2d-2072-44b3-a4a9-40e4a2c3cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  helpgetpredict(batch,candidate_logits,fastbuffer,topk=2,choices = 5):\n",
    "    indices =  0 \n",
    "    output = torch.tensor([])\n",
    "    for i in range(len(fastbuffer)):\n",
    "        ###1.首先取所有接受结果的topK\n",
    "        _,topK = candidate_logits[indices][i].topk(k=topk,dim=-1)\n",
    "        ###2.然后将fastbuffer对应位置token和topK进行比较，查看是否有合适的\n",
    "        mask = torch.isin(fastbuffer[i], topK)\n",
    "        ##判断，如果失败就直接退出\n",
    "        \n",
    "        ###3.更新indices，这里首先确定在fastbuffer的位置，然后由于batch的原因，所以需要更新buffer的位置\n",
    "        \n",
    "        if mask.sum() == 0: \n",
    "            indices=indices[0]\n",
    "             \n",
    "            for j in range(i):\n",
    "                indices = math.floor(indices/choices)\n",
    "                output =  torch.cat((fastbuffer[len(fastbuffer)-2-j][indices],output),dim=-1)\n",
    "            ##将当前logits,选择top1即可 \n",
    "            lasttoken = topK[0]\n",
    "            output = torch.cat((output,lasttoken),dim=-1)\n",
    "            return output  \n",
    "        indices = torch.nonzero(mask)\n",
    "        indices = indices*(len(fastbuffer[i])-i)*choices\n",
    "    indices= indices[0]\n",
    "    output =  torch.cat((output,fastbuffer[-1][indices]),dim=-1) \n",
    "    for j in range(len(fastbuffer)):\n",
    "            \n",
    "            indices = math.floor(indices/choices)\n",
    "            output =  torch.cat((fastbuffer[len(fastbuffer)-2-j][indices],output),dim=-1)\n",
    "    return output    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "665d0ccb-ec69-40d5-bb72-b74237592a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_predict(model,input_ids = None ,candidate = None ,attention_mask=None,past_key_values= None,topk=2):\n",
    "     \"\"\"candidate 是所有可能token序列片段\n",
    "     \"\"\"\n",
    "     aclength = 0\n",
    "     totallen = len(candidate[0])\n",
    "     best_candidate = torch.tensor([])\n",
    "     for i in candidate:\n",
    "        \n",
    "         input = torch.cat((input_ids,i),dim=-1)\n",
    "         outputs = model.base_model.model(\n",
    "                        input_ids=input.unsqueeze(0),\n",
    "                        attention_mask=attention_mask,\n",
    "                        past_key_values=past_key_values\n",
    "                    )\n",
    "         orig = model.base_model.lm_head(outputs[0])\n",
    "         \n",
    "         count = 0\n",
    "         for j in range(totallen):\n",
    "             _,tk = orig[0][j].topk(k=topk , dim=-1)\n",
    "             \n",
    "             if sum(tk.eq(i[j])) : count = count+1\n",
    "             else: break\n",
    "         if count > aclength: \n",
    "             aclength = count\n",
    "             best_candidate = i[:aclength]\n",
    "         if aclength == totallen:\n",
    "             return best_candidate\n",
    "     return best_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cad2d777-9825-401d-893c-580c56ce61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input,max_length = 100,choices=3,max_predictlen = 5,topk=3):\n",
    "    input = tokenizer([inputs])\n",
    "    input_ids = torch.tensor(input.input_ids)\n",
    "    attention_mask = torch.tensor(input.attention_mask)\n",
    "    past_key_values = None\n",
    "    output = input_ids \n",
    "    count = 0 \n",
    "    last_hs = None\n",
    "    fastbuffer =None\n",
    "    alltoken = []\n",
    "    ##initial\n",
    "     \n",
    "    ####\n",
    "    for i in range(max_length):\n",
    "        outputs = medusa_lm_head.base_model.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    past_key_values=past_key_values\n",
    "                )\n",
    "        #####回退使用无损加速策略,如果是top5就选择输出，就这样  \n",
    "        if last_hs is None : last_hs = outputs[0]\n",
    "        else :last_hs = torch.cat((last_hs,outputs[0]),dim=-2)\n",
    "        orig = medusa_lm_head.base_model.lm_head(outputs[0])        \n",
    "        past_key_values = outputs['past_key_values']\n",
    "        input_ids = torch.argmax(orig[0][-1]).unsqueeze(0).unsqueeze(0)\n",
    "        attention_mask = torch.cat((attention_mask,attention_mask[:,0].unsqueeze(0)),dim=-1)\n",
    "        output = torch.cat((output,input_ids),dim=-1) \n",
    "        #import pdb;pdb.set_trace();\n",
    "        candidate, fastbuffer=headgenerate(medusa_lm_head,input_ids=output,attention_mask=attention_mask,last_hs = last_hs,max_len =max_predictlen,\n",
    "                                           choices = choices)\n",
    "        \n",
    "        \"\"\"evaluate candiate\"\"\"\n",
    "        #import pdb;pdb.set_trace();\n",
    "        attention_mask_ev = torch.cat((attention_mask,attention_mask[:,0:max_predictlen]),dim=-1)\n",
    "        best_candidate = naive_predict(medusa_lm_head,input_ids = input_ids[0],candidate = candidate[:,len(output[0]):] ,attention_mask=attention_mask_ev,\n",
    "                                       past_key_values= past_key_values,topk=topk)\n",
    "            \n",
    "        \"\"\"renew kv,atmk , input,output\"\"\"    \n",
    "        print(\"速度{}\".format(len(best_candidate)+1))\n",
    "        #print(candidate.shape)\n",
    "        ####这里直接将预测的token放入\n",
    "        #import pdb;pdb.set_trace();\n",
    "        if len(best_candidate) > 0:\n",
    "            input_ids = torch.cat((input_ids,best_candidate.unsqueeze(0)),dim=-1)\n",
    "            output =  torch.cat((output,best_candidate.unsqueeze(0)),dim=-1)  \n",
    "            attention_mask = torch.cat((attention_mask,attention_mask[:,0:len(best_candidate)]),dim=-1)\n",
    "            tokenizer.decode(output[0])\n",
    "\n",
    "    print(count)\n",
    "    print(tokenizer.decode(output[0]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f05435e-0a80-44ec-8e20-8ebc0e1651d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "速度2\n",
      "速度2\n",
      "速度5\n",
      "速度1\n",
      "速度4\n",
      "速度4\n",
      "速度4\n",
      "速度2\n",
      "速度6\n",
      "速度2\n",
      "0\n",
      "<s>Please tell me a story about llama:\n",
      "\n",
      "Once upon a time, there was a llama named Luna.\n",
      "\n",
      "Luna was a very curious llama.\n",
      "\n",
      "Luna was\n"
     ]
    }
   ],
   "source": [
    "output = generate(inputs,max_length =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5621e98-c9c3-4e2d-bebd-d87aac872f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Please tell me a story about llama:\\n\\nOnce upon a time, there was a llama named Luna.\\n\\nLuna was a very curious llama.\\n\\nLuna was'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd24ca-2ffb-4b9c-b3a7-11b55535e60b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
