{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a602fc2b-cebf-4ad4-b46a-252cbd1d4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is based on tatsu-lab/stanford_alpaca. Below is the original copyright:\n",
    "#\n",
    "from transformers.trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19410712-5f58-467e-9943-0534bef0b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "\n",
    "# Adapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer, BitsAndBytesConfig\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "\n",
    "from fastchat.conversation import SeparatorStyle\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from medusa.model.medusa_model import MedusaModel, MedusaConfig,SingleMedusa\n",
    "import torch.nn.functional as F\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "\n",
    "\n",
    "# Customized for training Medusa heads\n",
    "class CustomizedTrainer(Trainer):\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on `model` using `inputs`.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "            ignore_keys (`List[str]`, *optional*):\n",
    "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
    "                gathering predictions.\n",
    "\n",
    "        Return:\n",
    "            Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
    "            logits and labels (each being optional).\n",
    "        \"\"\"\n",
    "        has_labels = False if len(self.label_names) == 0 else all(inputs.get(k) is not None for k in self.label_names)\n",
    "        # For CLIP-like models capable of returning loss values.\n",
    "        # If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`\n",
    "        # is `True` in `model.forward`.\n",
    "        return_loss = inputs.get(\"return_loss\", None)\n",
    "        if return_loss is None:\n",
    "            return_loss = self.can_return_loss\n",
    "        loss_without_labels = True if len(self.label_names) == 0 and return_loss else False\n",
    "\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
    "        if has_labels or loss_without_labels:\n",
    "            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
    "            if len(labels) == 1:\n",
    "                labels = labels[0]\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if is_sagemaker_mp_enabled():\n",
    "                raw_outputs = smp_forward_only(model, inputs)\n",
    "                if has_labels or loss_without_labels:\n",
    "                    if isinstance(raw_outputs, dict):\n",
    "                        loss_mb = raw_outputs[\"loss\"]\n",
    "                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "                    else:\n",
    "                        loss_mb = raw_outputs[0]\n",
    "                        logits_mb = raw_outputs[1:]\n",
    "\n",
    "                    loss = loss_mb.reduce_mean().detach().cpu()\n",
    "                    logits = smp_nested_concat(logits_mb)\n",
    "                else:\n",
    "                    loss = None\n",
    "                    if isinstance(raw_outputs, dict):\n",
    "                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys)\n",
    "                    else:\n",
    "                        logits_mb = raw_outputs\n",
    "                    logits = smp_nested_concat(logits_mb)\n",
    "            else:\n",
    "                if has_labels or loss_without_labels:\n",
    "                    with self.compute_loss_context_manager():\n",
    "                        loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "                    loss = loss.mean().detach()\n",
    "                    import pdb;pdb.set_trace()\n",
    "                    if isinstance(outputs, dict):\n",
    "                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "                    else:\n",
    "                        logits = outputs[:]\n",
    "                else:\n",
    "                    loss = None\n",
    "                    with self.compute_loss_context_manager():\n",
    "                        outputs = model(**inputs)\n",
    "                    import pdb;pdb.set_trace()\n",
    "                    if isinstance(outputs, dict):\n",
    "                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n",
    "                    else:\n",
    "                        logits = outputs\n",
    "                    # TODO: this needs to be fixed and made cleaner later.\n",
    "                    if self.args.past_index >= 0:\n",
    "                        self._past = outputs[self.args.past_index - 1]\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        logits = nested_detach(logits)\n",
    "        if len(logits) == 1:\n",
    "            logits = logits[0]\n",
    "        import pdb;pdb.set_trace()\n",
    "        return (loss, logits, labels)\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # DDP will give us model.module\n",
    "        if hasattr(model, \"module\"):\n",
    "            medusa = model.module.medusa\n",
    "        else:\n",
    "            medusa = model.medusa\n",
    "\n",
    "        logits = model(\n",
    "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        logits =logits['logits']\n",
    "        \n",
    "        labels = inputs[\"labels\"]\n",
    "        # Shift so that tokens < n predict n\n",
    "        loss = 0\n",
    "        loss_fct =CrossEntropyLoss()\n",
    "        log = {}\n",
    "        #logits = torch.clamp(logits, min=1e-7, max=100 - 1e-7)\n",
    "        for i in range(medusa):\n",
    "            #########修改后#######\n",
    "            # medusa_logits = logits[i, :, : -1].contiguous()\n",
    "            \n",
    "            # medusa_labels = labels[...,  2:].contiguous()\n",
    "            ######原medusa#########\n",
    "            \n",
    "            medusa_logits = logits[i, :, : ].contiguous()\n",
    "            \n",
    "            medusa_labels = labels[...,  4:].contiguous()\n",
    "            medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "            medusa_labels = medusa_labels.view(-1)\n",
    "            \n",
    "            medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "            \n",
    "            #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=100 - 1e-7)\n",
    "           \n",
    "            loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "            loss += loss_i\n",
    "            not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "            medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "            # Add top-k accuracy\n",
    "            for k in range(1, 6):\n",
    "                _, topk = medusa_logits.topk(k, dim=-1)\n",
    "                topk = topk[not_ignore]\n",
    "                correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "                log[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "        \n",
    "            \n",
    "            log[f\"medusa{i}_loss\"] = loss_i.item()\n",
    "            #log[f\"medusa{i}_loss_7\"] = loss_i_7.item()\n",
    "        self.log(log)\n",
    "        return (loss, logits) if return_outputs else loss\n",
    "    \n",
    "\n",
    " \n",
    "    # def compute_metrics(pred):\n",
    "    #     labels,logits = pred.label_ids\n",
    "    #     logits = pred.predictions\n",
    "    #     medusa_logits = logits[i, :, : -1].contiguous()\n",
    "            \n",
    "    #     medusa_labels = labels[...,  1:].contiguous()\n",
    "    #     medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "    #     medusa_labels = medusa_labels.view(-1)\n",
    "        \n",
    "    #     medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "        \n",
    "    #     #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=1 - 1e-7)\n",
    "       \n",
    "    #     loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "    #     loss += loss_i\n",
    "    #     not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "    #     medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "    #     # Add top-k accuracy\n",
    "    #     for k in range(1, 6):\n",
    "    #         _, topk = medusa_logits.topk(k, dim=-1)\n",
    "    #         topk = topk[not_ignore]\n",
    "    #         correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "    #         log[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "    #         res[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "    \n",
    "        \n",
    "    #     log[f\"medusa{i}_loss\"] = loss_i.item()\n",
    "\n",
    "        \n",
    "    #     return log\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"lmsys/vicuna-7b-v1.3\")\n",
    "    load_in_4bit: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Load in 4 bit.\"},\n",
    "    )\n",
    "    load_in_8bit: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Load in 8 bit.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(\n",
    "        default=\"sharegpt_clean.json\",\n",
    "        metadata={\"help\": \"Path to the training data.\"},\n",
    "    )\n",
    "    eval_data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n",
    "    )\n",
    "    lazy_preprocess: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=2048,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    medusa_num_heads: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Number of Medusa heads.\"},\n",
    "    )\n",
    "    medusa_num_layers: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Number of layers for each Medusa head.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "local_rank = None\n",
    "\n",
    "\n",
    "def rank0_print(*args):\n",
    "    if local_rank == 0:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conv = get_conversation_template(\"vicuna\")\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}, {j}, {role}, {conv.roles[j % 2]}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "    input_ids = tokenizer(\n",
    "        conversations,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    targets = input_ids.clone()\n",
    "\n",
    "    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n",
    "\n",
    "    # Mask targets. Only compute loss on the assistant outputs.\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        turns = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_TOKEN_ID\n",
    "        for i, turn in enumerate(turns):\n",
    "            if turn == \"\":\n",
    "                break\n",
    "            turn_len = len(tokenizer(turn).input_ids)\n",
    "\n",
    "            parts = turn.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            # \"-2\" is hardcoded for the LLaMA tokenizer to make the offset correct.\n",
    "            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            # Ignore the user instructions\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n",
    "            cur_len += turn_len\n",
    "\n",
    "        target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "        if False:  # Inspect and check the correctness of masking\n",
    "            z = target.clone()\n",
    "            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
    "            rank0_print(tokenizer.decode(z))\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_TOKEN_ID\n",
    "                rank0_print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "                    f\" (ignored)\"\n",
    "                )\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        rank0_print(\"Formatting inputs...\")\n",
    "        sources = [example[\"conversations\"] for example in raw_data]\n",
    "        data_dict = preprocess(sources, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )\n",
    "\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.raw_data = raw_data\n",
    "        self.cached_data_dict = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        if i in self.cached_data_dict:\n",
    "            return self.cached_data_dict[i]\n",
    "\n",
    "        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer)\n",
    "        ret = dict(\n",
    "            input_ids=ret[\"input_ids\"][0],\n",
    "            labels=ret[\"labels\"][0],\n",
    "            attention_mask=ret[\"attention_mask\"][0],\n",
    "        )\n",
    "        self.cached_data_dict[i] = ret\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "def make_supervised_data_module(\n",
    "    tokenizer: transformers.PreTrainedTokenizer, data_args\n",
    ") -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    dataset_cls = (\n",
    "        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n",
    "    )\n",
    "    rank0_print(\"Loading data...\")\n",
    "\n",
    "    train_json = json.load(open(data_args.data_path, \"r\"))\n",
    "    train_dataset = dataset_cls(train_json, tokenizer=tokenizer)\n",
    "\n",
    "    if data_args.eval_data_path:\n",
    "        eval_json = json.load(open(data_args.eval_data_path, \"r\"))\n",
    "        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer)\n",
    "    else:\n",
    "        eval_dataset = None\n",
    "\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016c268d-0a2f-49f7-b739-793a424c258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "global local_rank\n",
    "import transformers \n",
    "#from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    local_rank=0,\n",
    "    model_max_length=1024 ,\n",
    "    medusa_num_heads = 1 ,\n",
    "    medusa_num_layers =  1 ,\n",
    "    output_dir= './test', \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 1 ,\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=1e-3, \n",
    "    weight_decay=0.0,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    fp16=True, #对应--bf16\n",
    "    tf32=True,\n",
    "    \n",
    ")\n",
    "#from transformers import DataArguments\n",
    "\n",
    "data_args = DataArguments(\n",
    "    data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "    eval_data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "    lazy_preprocess= True \n",
    ")\n",
    "#from transformers import ModelArguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    \n",
    "    model_name_or_path=\"../../../../../model/TinyLlama-1.1B-Chat-v0.6\",\n",
    "    #model_max_length=2048,\n",
    "    #lazy_preprocess=True,\n",
    "    # medusa_num_heads=3,\n",
    "    # medusa_num_layers=1\n",
    ")\n",
    "\n",
    "local_rank =0 # training_args.local_rank\n",
    "\n",
    "# Set RoPE scaling factor\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    ")\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len and training_args.model_max_length > orig_ctx_len:\n",
    "    scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n",
    "    config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "config.use_cache = False\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7baa1fb8-2f65-46f1-a870-69009ae5b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca29f9e-ead5-4cc3-b95f-5222149b07ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../../../../model/TinyLlama-1.1B-Chat-v0.6 and are newly initialized: ['model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config if model_args.load_in_4bit else None,\n",
    "        load_in_4bit=model_args.load_in_4bit,\n",
    "        load_in_8bit=model_args.load_in_8bit,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fcb30d-252f-4ae0-9f60-f055a08c4a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8603169c-d3b5-45ca-a71a-885125d51c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ../../../../../model/TinyLlama-1.1B-Chat-v0.6\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "#model_name = '../../../../idea6_3fastlayer_t1_skipbert_medusa_mlp_vicuna-7b-v1.3_medusa_1_lr_0.0001_layers_1'\n",
    "#medusa_lm_head = MedusaModel.from\n",
    "# for param in medusa_lm_head.base_model.parameters():\n",
    "#         param.requires_grad = False\n",
    "medusa_lm_head = MedusaModel(\n",
    "        model,\n",
    "        medusa_num_heads=training_args.medusa_num_heads,\n",
    "        medusa_num_layers=training_args.medusa_num_layers,\n",
    "        base_model_name_or_path=model_args.model_name_or_path\n",
    "    )\n",
    "training_args.output_dir = f\"{training_args.output_dir}_medusa_mlp_{model_args.model_name_or_path.split('/')[-1]}_medusa_{training_args.medusa_num_heads}_lr_{training_args.learning_rate}_layers_{training_args.medusa_num_layers}\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a46321b7-9030-4412-ab1c-21ba67168d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ResBlock(\n",
       "    (linear): Linear(in_features=6144, out_features=6144, bias=True)\n",
       "    (act): SiLU()\n",
       "  )\n",
       "  (1): Linear(in_features=6144, out_features=2048, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medusa_lm_head.fast_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49675b6e-c167-41dd-bb4c-2e6cac15e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"a a a a a\",\"are are are how are\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fdd87ec-4741-45b5-a9e7-165fd62e012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputid = torch.tensor(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20a288fb-091c-4bd1-81e6-34e9ee4f17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = medusa_lm_head.base_model.model(input_ids= inputid,attention_mask =   torch.tensor(inputs['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bceceada-c659-40fd-a6d2-1904b0b00c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a437891-030d-4e84-98f6-f24fa97fb8e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "inputs['attention_mask'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8665b4a8-81cb-46c0-a07a-8f1bdf8dfc87",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'medusa_lm_head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[43mmedusa_lm_head\u001b[49m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens(inputid)\n\u001b[1;32m      2\u001b[0m embedtrigram \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((embed[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m],embed[:,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],embed[:,\u001b[38;5;241m2\u001b[39m:]),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m embed \u001b[38;5;241m=\u001b[39m medusa_lm_head\u001b[38;5;241m.\u001b[39mfast_layer1(embedtrigram )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'medusa_lm_head' is not defined"
     ]
    }
   ],
   "source": [
    "embed = medusa_lm_head.base_model.model.embed_tokens(inputid)\n",
    "embedtrigram = torch.cat((embed[:,:-2],embed[:,1:-1],embed[:,2:]),dim=-1)\n",
    "embed = medusa_lm_head.fast_layer1(embedtrigram )\n",
    "from modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n",
    "batch_size, seq_length = embed.shape[:2]\n",
    "attention_mask = _prepare_4d_causal_attention_mask(\n",
    "                 torch.tensor(inputs['attention_mask'])[:,:-2], (batch_size, seq_length), embed, 0\n",
    "            )\n",
    "position_ids = torch.arange(\n",
    "                0, seq_length , dtype=torch.long\n",
    "            )\n",
    "position_ids = position_ids.unsqueeze(0)\n",
    "output2 = medusa_lm_head.base_model.model.layers[0](embed,attention_mask = attention_mask,position_ids= position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b935cada-398c-476d-91d6-34f936223477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0396,  0.0388,  0.0359,  ...,  0.0176, -0.0042,  0.0237],\n",
       "          [-0.0391,  0.0610,  0.0240,  ...,  0.0157,  0.0354,  0.0288],\n",
       "          [-0.0439,  0.0684,  0.0267,  ...,  0.0222,  0.0454,  0.0282],\n",
       "          [-0.0469,  0.0732,  0.0276,  ...,  0.0258,  0.0498,  0.0276]],\n",
       " \n",
       "         [[-0.0024,  0.1211, -0.0601,  ...,  0.0344, -0.0192,  0.0586],\n",
       "          [ 0.0239,  0.0835, -0.0537,  ...,  0.0137, -0.0084,  0.0361],\n",
       "          [ 0.0698,  0.0540, -0.0087,  ...,  0.0383, -0.0181,  0.0292],\n",
       "          [ 0.0183,  0.0344, -0.0505,  ...,  0.0483,  0.0150, -0.0181]]],\n",
       "        dtype=torch.bfloat16, grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cbcec7f-d98a-4f07-9cac-5dfe0536e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedtrigram = torch.cat((embed[:,:-2],embed[:,1:-1],embed[:,2:]),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0509c5c-b102-4089-b7b0-e89617caf8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6144])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedtrigram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce05b82e-a0d5-4fcc-833f-f613e77576ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "medusa_lm_head.fast_layer1 =medusa_lm_head.fast_layer1.to(medusa_lm_head.base_model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30dfef68-35aa-4b26-82ff-c9871ff9f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = medusa_lm_head.fast_layer1(embedtrigram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44eb48f8-78a6-4ad1-82ce-915bb12cc9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2048])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56114214-1baf-4fdb-9ded-9ad8186bdd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 2048])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e20ffbc-c68b-4231-8af9-90fb87f77223",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = embed.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd96a56c-2fca-4cf6-8bd1-456e5c185080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2048]), torch.Size([4, 2048]), torch.Size([4, 2048]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed[0][:-2].shape,embed[0][1:-1].shape,embed[0][2:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf724813-8700-4a30-b5db-f0d7c2769708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1, 263, 263, 263, 263, 263],\n",
       "        [  1, 526, 526, 526, 920, 526]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "418debe4-ca3e-42ea-b29c-cec1e84a566c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1, 263, 263, 263])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputid[0,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf3e65be-7fe0-4d22-bb6a-e490b803b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.nn import  MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "822d2bc3-eccd-4273-b734-f40ed09f2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct =MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3d1848e-c25f-48be-8c17-1efce929fe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaRMSNorm()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medusa_lm_head.base_model.model.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ca74a94-01c0-4327-98fe-92db6675fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "427cfc01-78d1-4751-ae1f-f17054786b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "newhs = torch.cat((inputid[:,:-2].unsqueeze(0),inputid[:,1:-1].unsqueeze(0),inputid[:,2:].unsqueeze(0)),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bba15f4-862c-4c5e-9c38-d5fbb66ff325",
   "metadata": {},
   "outputs": [],
   "source": [
    "newhs = torch.transpose(newhs,dim0=0,dim1=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1de889c7-adc8-4c16-a34b-57f1a2c4bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "newhs = torch.transpose(newhs,dim0=0,dim1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb6bfb5f-c35a-4e56-8676-722e12a53550",
   "metadata": {},
   "outputs": [],
   "source": [
    "newhs= torch.flatten(newhs,end_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37a46501-53e7-4788-881e-29c5e10ea442",
   "metadata": {},
   "outputs": [],
   "source": [
    "newhs1 =  medusa_lm_head.base_model.model(input_ids= newhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8244897-258b-4d5e-8c85-c773c2bde234",
   "metadata": {},
   "outputs": [],
   "source": [
    "newhs1 = newhs1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0343911d-2253-43a3-ae56-ed7d2b0e47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "newhs1 = newhs1.view((batch_size,-1,3,2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fceb378d-8424-471b-9c3f-09b47a218c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "newhs2 =newhs1[:,:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7de767b8-9420-4ba1-ab01-e880da043582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2048])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newhs2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0ea6b82-a95f-4143-b579-34240616d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask2 =torch.full((seq_length, seq_length), -3.4028e+38) + torch.diag(torch.zeros(seq_length)+3.4028e+38-1)\n",
    "attention_mask3 = torch.cat((attention_mask[0,0,:,:],attention_mask2),dim=-1)\n",
    "attention_mask3 = torch.cat((attention_mask3[:,:-1],attention_mask3[0:-1,:-1]),dim=-2).unsqueeze(0).unsqueeze(0)\n",
    "attention_mask3 = attention_mask3.repeat([batch_size,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d16c37b-43c4-454a-8a0b-d76e856647e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.arange(0, seq_length, dtype=torch.long )\n",
    "position_ids2 = torch.arange(1, seq_length , dtype=torch.long)\n",
    "position_ids2 = torch.cat((position_ids,position_ids2),dim=-1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5c7ca2a-62d9-4a0c-8b68-17185f9605a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed2 = torch.cat((res[0],embed[:,1:]),dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30cb2f59-9937-4f73-a979-a6217c384fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = medusa_lm_head.base_model.model.layers[0](embed2,attention_mask = attention_mask3,position_ids= position_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f47759ac-6038-47bb-85c5-5c85167cb336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2048])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3[0][:,-seq_length+1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b3dd854-df65-4832-b8a2-e82cd0c97e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Formatting inputs...Skip in lazy mode\n",
      "Formatting inputs...Skip in lazy mode\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "#compute metrics\n",
    "def compute_metrics(pred):\n",
    "        logits,labels = pred\n",
    "        #logits = pred.predictions\n",
    "        #print(logits.shape)\n",
    "        import pdb; pdb.set_trace()\n",
    "        #print(labels.shape)\n",
    "        medusa_logits = logits[0,:, : ].contiguous()\n",
    "            \n",
    "        medusa_labels = labels[...,  1:].contiguous()\n",
    "        medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "        medusa_labels = medusa_labels.view(-1)\n",
    "        \n",
    "        medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "        \n",
    "        #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=1 - 1e-7)\n",
    "       \n",
    "        loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "        loss += loss_i\n",
    "        not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "        medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "        # Add top-k accuracy\n",
    "        for k in range(1, 6):\n",
    "            _, topk = medusa_logits.topk(k, dim=-1)\n",
    "            topk = topk[not_ignore]\n",
    "            correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "            log[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "            #res[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "    \n",
    "        \n",
    "        log[f\"medusa{i}_loss\"] = loss_i.item()\n",
    "\n",
    "        \n",
    "        return log\n",
    "# Generate Medusa config for pushing to HF hub\n",
    "medusa_config = MedusaConfig(\n",
    "    medusa_num_heads=training_args.medusa_num_heads,\n",
    "    medusa_num_layers=training_args.medusa_num_layers,\n",
    "    base_model_name_or_path=model_args.model_name_or_path,\n",
    ")\n",
    "\n",
    "# Save Medusa config\n",
    "medusa_config.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "# Start trainner\n",
    "trainer = CustomizedTrainer(\n",
    "    model=medusa_lm_head, tokenizer=tokenizer, args=training_args,compute_metrics = compute_metrics , **data_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e89c9-c462-4b73-ae09-9e206728f3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791531e1-3a1c-4555-bef8-21aeed36e6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 4, 1020, 32000])) that is different to the input size (torch.Size([4, 1020, 32000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myu13668962105\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/liyunhao/nlptest/medusa/Medusa/medusa/train/wandb/run-20231226_162309-w0x7fz81</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yu13668962105/huggingface/runs/w0x7fz81' target=\"_blank\">./test</a></strong> to <a href='https://wandb.ai/yu13668962105/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yu13668962105/huggingface' target=\"_blank\">https://wandb.ai/yu13668962105/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yu13668962105/huggingface/runs/w0x7fz81' target=\"_blank\">https://wandb.ai/yu13668962105/huggingface/runs/w0x7fz81</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_225059/3234715425.py\u001b[0m(125)\u001b[0;36mprediction_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    123 \u001b[0;31m                    \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    124 \u001b[0;31m                    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 125 \u001b[0;31m                    \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    126 \u001b[0;31m                        \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore_keys\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    127 \u001b[0;31m                    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_225059/3234715425.py\u001b[0m(149)\u001b[0;36mprediction_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    147 \u001b[0;31m            \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    148 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 149 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    150 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    151 \u001b[0;31m        \u001b[0;31m# DDP will give us model.module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 4, 1020, 32000])) that is different to the input size (torch.Size([4, 1020, 32000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_225059/3234715425.py\u001b[0m(125)\u001b[0;36mprediction_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    123 \u001b[0;31m                    \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    124 \u001b[0;31m                    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 125 \u001b[0;31m                    \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    126 \u001b[0;31m                        \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore_keys\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    127 \u001b[0;31m                    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_225059/3234715425.py\u001b[0m(149)\u001b[0;36mprediction_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    147 \u001b[0;31m            \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    148 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 149 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    150 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    151 \u001b[0;31m        \u001b[0;31m# DDP will give us model.module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  logits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-21.2656, -21.9531,  -1.5957,  ...,  -8.6406, -17.3125,  -4.3594],\n",
      "          [-24.5625, -24.2188,   1.5781,  ..., -10.2344, -21.6562,  -9.1328],\n",
      "          [-30.2188, -30.5156,   1.1182,  ..., -12.5938, -24.3281, -11.6328],\n",
      "          ...,\n",
      "          [-27.2656, -28.1562,   3.1309,  ..., -19.9219, -21.2969, -21.7188],\n",
      "          [-25.9688, -25.6875,   2.1016,  ..., -20.0312, -20.9688, -20.8906],\n",
      "          [-25.5312, -26.1562,   3.0371,  ..., -21.1875, -19.6562, -21.0625]],\n",
      "\n",
      "         [[-21.2656, -21.9531,  -1.5957,  ...,  -8.6406, -17.3125,  -4.3594],\n",
      "          [-24.5625, -24.2188,   1.5781,  ..., -10.2344, -21.6562,  -9.1328],\n",
      "          [-30.2188, -30.5156,   1.1182,  ..., -12.5938, -24.3281, -11.6328],\n",
      "          ...,\n",
      "          [-25.8438, -26.1406,   9.1484,  ..., -13.5547, -22.6094, -18.2812],\n",
      "          [-25.4219, -25.8594,   8.4062,  ..., -13.3906, -22.2969, -17.8906],\n",
      "          [-25.7188, -26.7188,   8.1641,  ..., -13.7812, -22.2656, -17.5781]],\n",
      "\n",
      "         [[-21.2656, -21.9531,  -1.5957,  ...,  -8.6406, -17.3125,  -4.3594],\n",
      "          [-24.5625, -24.2188,   1.5781,  ..., -10.2344, -21.6562,  -9.1328],\n",
      "          [-30.2188, -30.5156,   1.1182,  ..., -12.5938, -24.3281, -11.6328],\n",
      "          ...,\n",
      "          [-24.7656, -25.6406,   3.3066,  ..., -14.3906, -16.7812, -17.6562],\n",
      "          [-21.4688, -23.2812,   3.6543,  ..., -13.3516, -17.0781, -16.4219],\n",
      "          [-24.3125, -25.0938,   2.4883,  ..., -13.9531, -16.3750, -22.1094]],\n",
      "\n",
      "         [[-21.2656, -21.9531,  -1.5957,  ...,  -8.6406, -17.3125,  -4.3594],\n",
      "          [-24.5625, -24.2188,   1.5781,  ..., -10.2344, -21.6562,  -9.1328],\n",
      "          [-30.2188, -30.5156,   1.1182,  ..., -12.5938, -24.3281, -11.6328],\n",
      "          ...,\n",
      "          [-29.6406, -32.2500,   3.8809,  ..., -22.4062, -25.2031, -22.4688],\n",
      "          [-29.7031, -32.0000,   5.7461,  ..., -19.5469, -23.2969, -21.6562],\n",
      "          [-28.8906, -30.0156,   4.6875,  ..., -20.8750, -23.3281, -20.6719]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_225059/3234715425.py\u001b[0m(125)\u001b[0;36mprediction_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    123 \u001b[0;31m                    \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    124 \u001b[0;31m                    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 125 \u001b[0;31m                    \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    126 \u001b[0;31m                        \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore_keys\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    127 \u001b[0;31m                    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_225059/3234715425.py\u001b[0m(149)\u001b[0;36mprediction_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    147 \u001b[0;31m            \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    148 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 149 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    150 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    151 \u001b[0;31m        \u001b[0;31m# DDP will give us model.module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1, 1020, 32000])) that is different to the input size (torch.Size([1, 1020, 32000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_225059/3234715425.py\u001b[0m(125)\u001b[0;36mprediction_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    123 \u001b[0;31m                    \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    124 \u001b[0;31m                    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 125 \u001b[0;31m                    \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    126 \u001b[0;31m                        \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore_keys\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    127 \u001b[0;31m                    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_225059/3234715425.py\u001b[0m(149)\u001b[0;36mprediction_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    147 \u001b[0;31m            \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    148 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 149 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    150 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    151 \u001b[0;31m        \u001b[0;31m# DDP will give us model.module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_225059/775524108.py\u001b[0m(10)\u001b[0;36mcompute_metrics\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      8 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      9 \u001b[0;31m        \u001b[0;31m#print(labels.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 10 \u001b[0;31m        \u001b[0mmedusa_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0mmedusa_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pred.preditions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** AttributeError: 'EvalPrediction' object has no attribute 'preditions'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pred.predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -29.734375 ,  -28.953125 ,    7.8945312, ...,  -19.296875 ,\n",
      "           -29.40625  ,  -20.015625 ],\n",
      "         [ -29.84375  ,  -29.09375  ,    8.0078125, ...,  -19.28125  ,\n",
      "           -29.546875 ,  -20.40625  ],\n",
      "         [ -29.734375 ,  -29.171875 ,    8.3671875, ...,  -19.46875  ,\n",
      "           -29.8125   ,  -20.203125 ]],\n",
      "\n",
      "        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -27.0625   ,  -27.40625  ,    6.875    , ...,  -17.890625 ,\n",
      "           -23.515625 ,  -19.46875  ],\n",
      "         [ -26.90625  ,  -27.609375 ,    6.3515625, ...,  -17.890625 ,\n",
      "           -23.671875 ,  -19.96875  ],\n",
      "         [ -26.90625  ,  -27.578125 ,    6.7382812, ...,  -17.953125 ,\n",
      "           -22.625    ,  -19.75     ]],\n",
      "\n",
      "        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -27.734375 ,  -30.359375 ,    3.1640625, ...,  -20.25     ,\n",
      "           -20.484375 ,  -21.34375  ],\n",
      "         [ -29.375    ,  -31.359375 ,    5.53125  , ...,  -20.       ,\n",
      "           -22.84375  ,  -20.5625   ],\n",
      "         [ -26.78125  ,  -29.1875   ,    2.6132812, ...,  -19.625    ,\n",
      "           -21.515625 ,  -17.40625  ]],\n",
      "\n",
      "        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -27.984375 ,  -26.84375  ,    4.890625 , ...,  -14.25     ,\n",
      "           -21.171875 ,  -18.53125  ],\n",
      "         [ -27.328125 ,  -25.46875  ,    6.6992188, ...,  -14.5234375,\n",
      "           -20.078125 ,  -18.578125 ],\n",
      "         [ -24.828125 ,  -24.234375 ,    2.9375   , ...,  -14.6953125,\n",
      "           -17.796875 ,  -16.96875  ]]],\n",
      "\n",
      "\n",
      "       [[[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -27.265625 ,  -28.15625  ,    3.1308594, ...,  -19.921875 ,\n",
      "           -21.296875 ,  -21.71875  ],\n",
      "         [ -25.96875  ,  -25.6875   ,    2.1015625, ...,  -20.03125  ,\n",
      "           -20.96875  ,  -20.890625 ],\n",
      "         [ -25.53125  ,  -26.15625  ,    3.0371094, ...,  -21.1875   ,\n",
      "           -19.65625  ,  -21.0625   ]],\n",
      "\n",
      "        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -25.84375  ,  -26.140625 ,    9.1484375, ...,  -13.5546875,\n",
      "           -22.609375 ,  -18.28125  ],\n",
      "         [ -25.421875 ,  -25.859375 ,    8.40625  , ...,  -13.390625 ,\n",
      "           -22.296875 ,  -17.890625 ],\n",
      "         [ -25.71875  ,  -26.71875  ,    8.1640625, ...,  -13.78125  ,\n",
      "           -22.265625 ,  -17.578125 ]],\n",
      "\n",
      "        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -24.765625 ,  -25.640625 ,    3.3066406, ...,  -14.390625 ,\n",
      "           -16.78125  ,  -17.65625  ],\n",
      "         [ -21.46875  ,  -23.28125  ,    3.6542969, ...,  -13.3515625,\n",
      "           -17.078125 ,  -16.421875 ],\n",
      "         [ -24.3125   ,  -25.09375  ,    2.4882812, ...,  -13.953125 ,\n",
      "           -16.375    ,  -22.109375 ]],\n",
      "\n",
      "        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -29.640625 ,  -32.25     ,    3.8808594, ...,  -22.40625  ,\n",
      "           -25.203125 ,  -22.46875  ],\n",
      "         [ -29.703125 ,  -32.       ,    5.7460938, ...,  -19.546875 ,\n",
      "           -23.296875 ,  -21.65625  ],\n",
      "         [ -28.890625 ,  -30.015625 ,    4.6875   , ...,  -20.875    ,\n",
      "           -23.328125 ,  -20.671875 ]]],\n",
      "\n",
      "\n",
      "       [[[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -29.234375 ,  -28.78125  ,    4.5078125, ...,  -17.203125 ,\n",
      "           -19.375    ,  -18.65625  ],\n",
      "         [ -29.296875 ,  -28.515625 ,    6.15625  , ...,  -17.03125  ,\n",
      "           -21.765625 ,  -19.609375 ],\n",
      "         [ -28.90625  ,  -28.484375 ,    6.       , ...,  -18.171875 ,\n",
      "           -18.609375 ,  -18.140625 ]],\n",
      "\n",
      "        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -29.703125 ,  -28.96875  ,    6.6445312, ...,  -21.84375  ,\n",
      "           -23.671875 ,  -20.375    ],\n",
      "         [ -29.84375  ,  -29.34375  ,    6.5195312, ...,  -21.765625 ,\n",
      "           -23.28125  ,  -20.671875 ],\n",
      "         [ -29.734375 ,  -28.59375  ,    6.4804688, ...,  -21.8125   ,\n",
      "           -23.296875 ,  -20.859375 ]],\n",
      "\n",
      "        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -34.6875   ,  -35.5625   ,    4.9726562, ...,  -24.21875  ,\n",
      "           -27.546875 ,  -24.921875 ],\n",
      "         [ -35.4375   ,  -36.03125  ,    6.0742188, ...,  -26.03125  ,\n",
      "           -30.953125 ,  -26.828125 ],\n",
      "         [ -32.0625   ,  -33.625    ,    5.2148438, ...,  -25.15625  ,\n",
      "           -27.046875 ,  -23.078125 ]],\n",
      "\n",
      "        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,\n",
      "           -17.3125   ,   -4.359375 ],\n",
      "         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,\n",
      "           -21.65625  ,   -9.1328125],\n",
      "         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,\n",
      "           -24.328125 ,  -11.6328125],\n",
      "         ...,\n",
      "         [ -28.21875  ,  -27.75     ,    7.0585938, ...,  -13.359375 ,\n",
      "           -25.484375 ,  -19.359375 ],\n",
      "         [ -27.78125  ,  -26.875    ,    7.1484375, ...,  -13.5859375,\n",
      "           -25.03125  ,  -18.859375 ],\n",
      "         [ -27.90625  ,  -26.59375  ,    8.4296875, ...,  -13.2734375,\n",
      "           -25.28125  ,  -18.453125 ]]],\n",
      "\n",
      "\n",
      "       [[[ -21.3125   ,  -22.015625 ,   -1.6005859, ...,   -8.6796875,\n",
      "           -17.359375 ,   -4.390625 ],\n",
      "         [ -24.5625   ,  -24.234375 ,    1.5625   , ...,  -10.234375 ,\n",
      "           -21.671875 ,   -9.140625 ],\n",
      "         [ -30.265625 ,  -30.5625   ,    1.1201172, ...,  -12.625    ,\n",
      "           -24.375    ,  -11.6640625],\n",
      "         ...,\n",
      "         [ -31.25     ,  -32.875    ,    5.6601562, ...,  -18.484375 ,\n",
      "           -28.734375 ,  -20.203125 ],\n",
      "         [ -27.015625 ,  -29.359375 ,    3.1484375, ...,  -18.28125  ,\n",
      "           -25.625    ,  -18.640625 ],\n",
      "         [ -29.59375  ,  -30.96875  ,    4.3085938, ...,  -18.       ,\n",
      "           -27.8125   ,  -19.78125  ]],\n",
      "\n",
      "        [[-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         ...,\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ]],\n",
      "\n",
      "        [[-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         ...,\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ]],\n",
      "\n",
      "        [[-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         ...,\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ],\n",
      "         [-100.       , -100.       , -100.       , ..., -100.       ,\n",
      "          -100.       , -100.       ]]]], dtype=float32)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pred.predictions.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 1020, 32000)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pred.labels.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** AttributeError: 'EvalPrediction' object has no attribute 'labels'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pred.label.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** AttributeError: 'EvalPrediction' object has no attribute 'label'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pred.label_ids\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
      "       [ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
      "       [ -100,  -100,  -100, ...,   893, 29874,   313],\n",
      "       ...,\n",
      "       [ -100,  -100,  -100, ..., 29879,  1950,   393],\n",
      "       [ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
      "       [ -100,  -100,  -100, ...,   890,    13,    13]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pred.label_ids.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 1024)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pred.predictions.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 1020, 32000)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'contiguous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/transformers/trainer.py:2934\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2931\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2933\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2934\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2935\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2937\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2938\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2942\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2944\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/transformers/trainer.py:3222\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3218\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3219\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3220\u001b[0m         )\n\u001b[1;32m   3221\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3222\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3224\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdb\u001b[39;00m; pdb\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(labels.shape)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m medusa_logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m()\n\u001b[1;32m     12\u001b[0m medusa_labels \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,  \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     13\u001b[0m medusa_logits \u001b[38;5;241m=\u001b[39m medusa_logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'contiguous'"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79a645cd-68be-408a-b30e-91f3fe93cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_220651/775524108.py\u001b[0m(10)\u001b[0;36mcompute_metrics\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      8 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      9 \u001b[0;31m        \u001b[0;31m#print(labels.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 10 \u001b[0;31m        \u001b[0mmedusa_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0mmedusa_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pred.predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([], shape=(0, 1, 4, 1020, 32000), dtype=float32)\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.predict(data_module['eval_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c438838a-f820-451d-a294-e687f0667b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args.use_legacy_prediction_loop=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c2ef2-8767-4826-bbeb-b5416d34c291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_220651/775524108.py\u001b[0m(10)\u001b[0;36mcompute_metrics\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      8 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      9 \u001b[0;31m        \u001b[0;31m#print(labels.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 10 \u001b[0;31m        \u001b[0mmedusa_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0mmedusa_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a8dcbcd-bdb6-476b-974e-3aba04b7ac5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Trainer.evaluation_loop of <__main__.CustomizedTrainer object at 0x7ff0fc6ba6e0>>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluation_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d5108ea-9d78-4a58-8255-8243fe0e35c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomizedTrainer' object has no attribute 'gather_for_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather_for_metrics\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomizedTrainer' object has no attribute 'gather_for_metrics'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d71f6e79-910e-41bb-a832-09035cce8424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/transformers/trainer.py:1901\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1898\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1903\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/transformers/trainer.py:2226\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2224\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2226\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2229\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/transformers/trainer.py:2934\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2931\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2933\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2934\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2935\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2937\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2938\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2942\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2944\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/transformers/trainer.py:3123\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3120\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3122\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3123\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3124\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/transformers/trainer.py:3337\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   3336\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3337\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3338\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   3340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/var/conda/lib/python3.10/site-packages/torch/_tensor.py:930\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 930\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    932\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    939\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "41a02e99-d0c4-47b3-b07b-d91664055243",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 135\u001b[0m\n\u001b[1;32m    128\u001b[0m training_args\u001b[38;5;241m.\u001b[39moutput_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_medusa_mlp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_args\u001b[38;5;241m.\u001b[39mmodel_name_or_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_medusa_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args\u001b[38;5;241m.\u001b[39mmedusa_num_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args\u001b[38;5;241m.\u001b[39mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_layers_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args\u001b[38;5;241m.\u001b[39mmedusa_num_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Start trainner\u001b[39;00m\n\u001b[1;32m    134\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomizedTrainer(\n\u001b[0;32m--> 135\u001b[0m     model\u001b[38;5;241m=\u001b[39mmedusa_lm_head, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, args\u001b[38;5;241m=\u001b[39mtraining_args, compute_metrics \u001b[38;5;241m=\u001b[39m compute_metrics,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mdata_module\u001b[49m\n\u001b[1;32m    136\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_module' is not defined"
     ]
    }
   ],
   "source": [
    "class CustomizedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=True):\n",
    "        # DDP will give us model.module\n",
    "        if hasattr(model, \"module\"):\n",
    "            medusa = model.module.medusa\n",
    "        else:\n",
    "            medusa = model.medusa\n",
    "\n",
    "        logits = model(\n",
    "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        \n",
    "        labels = inputs[\"labels\"]\n",
    "        # Shift so that tokens < n predict n\n",
    "        loss = 0\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        log = {}\n",
    "        for i in range(medusa):\n",
    "            \n",
    "            medusa_logits = logits[i, :, : -1].contiguous()\n",
    "            \n",
    "            medusa_labels = labels[...,  2:].contiguous()\n",
    "            medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "            medusa_labels = medusa_labels.view(-1)\n",
    "            \n",
    "            medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "            \n",
    "            #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=1 - 1e-7)\n",
    "           \n",
    "            loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "            loss += loss_i\n",
    "            not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "            medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "            # Add top-k accuracy\n",
    "            for k in range(1, 6):\n",
    "                _, topk = medusa_logits.topk(k, dim=-1)\n",
    "                topk = topk[not_ignore]\n",
    "                correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "                log[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "            log[f\"medusa{i}_loss\"] = loss_i.item()\n",
    "            #log[f\"medusa{i}_loss_7\"] = loss_i_7.item()\n",
    "        #import pdb; pdb.set_trace()\n",
    "        self.log(log)\n",
    "        \n",
    "        return (loss, logits) #if return_outputs else loss\n",
    "def compute_metrics(pred):\n",
    "        \n",
    "        labels = pred.label_ids\n",
    "        logits = pred.predictions\n",
    "        #import pdb; pdb.set_trace()\n",
    "        print(pred)\n",
    "        medusa_logits = logits[0, :, : -1].contiguous()\n",
    "            \n",
    "        medusa_labels = labels[...,  2:].contiguous()\n",
    "        medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "        medusa_labels = medusa_labels.view(-1)\n",
    "        \n",
    "        medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "        \n",
    "        #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=1 - 1e-7)\n",
    "       \n",
    "        loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "        loss += loss_i\n",
    "        not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "        medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "        # Add top-k accuracy\n",
    "        for k in range(1, 6):\n",
    "            _, topk = medusa_logits.topk(k, dim=-1)\n",
    "            topk = topk[not_ignore]\n",
    "            correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "            log[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "            res[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "    \n",
    "        \n",
    "        log[f\"medusa{i}_loss\"] = loss_i.item()\n",
    "\n",
    "        \n",
    "        return log\n",
    "global local_rank\n",
    "import transformers \n",
    "#from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    local_rank=0,\n",
    "    model_max_length=1024 ,\n",
    "    medusa_num_heads = 1 ,\n",
    "    medusa_num_layers =  1 ,\n",
    "    output_dir= './test', \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 1 ,\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=1e-3, \n",
    "    weight_decay=0.0,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    fp16=True, #对应--bf16\n",
    "    tf32=True,\n",
    "    \n",
    ")\n",
    "#from transformers import DataArguments\n",
    "\n",
    "data_args = DataArguments(\n",
    "    data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/1280test.json\",\n",
    "    eval_data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/1280test.json\",\n",
    "    lazy_preprocess= True \n",
    ")\n",
    "#from transformers import ModelArguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    \n",
    "    model_name_or_path=\"../../../../../model/vicuna-7b-v1.3\",\n",
    "    #model_max_length=2048,\n",
    "    #lazy_preprocess=True,\n",
    "    # medusa_num_heads=3,\n",
    "    # medusa_num_layers=1\n",
    ")\n",
    "\n",
    "local_rank =0 \n",
    "\n",
    "training_args.output_dir = f\"{training_args.output_dir}_medusa_mlp_{model_args.model_name_or_path.split('/')[-1]}_medusa_{training_args.medusa_num_heads}_lr_{training_args.learning_rate}_layers_{training_args.medusa_num_layers}\"\n",
    "\n",
    "\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "# Start trainner\n",
    "trainer = CustomizedTrainer(\n",
    "    model=medusa_lm_head, tokenizer=tokenizer, args=training_args, compute_metrics = compute_metrics,**data_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e871d-598e-4ed0-a153-3cabf2b9f48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_13542/1513928275.py\u001b[0m(44)\u001b[0;36mcompute_loss\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     42 \u001b[0;31m            \u001b[0;31m#log[f\"medusa{i}_loss_7\"] = loss_i_7.item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 44 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     45 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     46 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#if return_outputs else loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  loss.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  logits.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 1023, 32000])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  logits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ -0.9927, -20.5625,  -2.8359,  ...,   0.4290,   0.9609,  -1.2070],\n",
      "          [-26.0625, -29.8906,  -3.9062,  ..., -33.0625, -17.0938, -24.0469],\n",
      "          [ -6.2305,  -5.8984,   4.7070,  ...,  -0.7690,  -4.9609,  -5.0312],\n",
      "          ...,\n",
      "          [ -2.1953, -29.7812,  10.4688,  ...,   2.3438,  -3.8945,  -2.3164],\n",
      "          [ -1.8789, -30.4688,  10.3984,  ...,   2.8223,  -3.4277,  -1.9736],\n",
      "          [ -1.5889, -30.9844,  10.0469,  ...,   3.3086,  -2.8164,  -1.5332]],\n",
      "\n",
      "         [[ -0.9927, -20.5625,  -2.8359,  ...,   0.4290,   0.9609,  -1.2070],\n",
      "          [-26.0625, -29.8906,  -3.9062,  ..., -33.0625, -17.0938, -24.0469],\n",
      "          [ -6.2305,  -5.8984,   4.7070,  ...,  -0.7690,  -4.9609,  -5.0312],\n",
      "          ...,\n",
      "          [ -5.5117,   0.7793,  21.2500,  ...,  -8.7656,  -9.6719,  -9.1328],\n",
      "          [ -5.4023,   0.6934,  20.9844,  ...,  -8.6562,  -9.5625,  -8.9922],\n",
      "          [ -5.2031,   0.4268,  20.6094,  ...,  -8.4609,  -9.4062,  -8.8203]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6962, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db69120a-e5cc-48e3-bafa-e4bb8f8b2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../../../../../data/ShareGPT_Vicuna_unfiltered/test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f67fd3d-5292-4547-9d63-f186abb15a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6862"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5cdd699b-27a9-44a9-81e8-eb5f898f4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = content[-1280:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4dbc85a9-634a-461a-a4e3-6bdf4b607b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../../../data/ShareGPT_Vicuna_unfiltered/1280test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "118cb196-59e5-4b01-bb06-0b09215e2b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset xsum/default to /home/liyunhao/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2941b5782b94253b9ef1e9e7a06e359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbca37b6ee4d45eb92d582b679840fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset xsum downloaded and prepared to /home/liyunhao/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4128c874f84aa6b7b91f8d41551096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"../../../../../data/xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88f078e7-5c63-4a95-b535-520bda5c43c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431fb579-fed3-41bd-9cdb-43d37d51e911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
