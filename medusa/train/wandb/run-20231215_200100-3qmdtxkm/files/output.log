Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../../../../model/TinyLlama-1.1B-Chat-v0.6 and are newly initialized: ['model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
path:  ../../../../../model/TinyLlama-1.1B-Chat-v0.6
Loading data...
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
path:  ../../../../../model/TinyLlama-1.1B-Chat-v0.6
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../../../../model/TinyLlama-1.1B-Chat-v0.6 and are newly initialized: ['model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading data...
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
> /tmp/ipykernel_169503/4075727162.py(7)compute_metrics()
      5         import pdb; pdb.set_trace()
      6         #print(labels.shape)
----> 7         medusa_logits = logits[0,:, : ].contiguous()
      8
      9         medusa_labels = labels[...,  1:].contiguous()
Token indices sequence length is longer than the specified maximum sequence length for this model (156 > 128). Running this sequence through the model will result in indexing errors
*** NameError: name 'logtis' is not defined
<transformers.trainer_utils.EvalPrediction object at 0x7f927c571b10>
array([], shape=(0, 1, 1, 125, 32000), dtype=float32)
*** AttributeError: 'EvalPrediction' object has no attribute 'labels'
array([], shape=(0, 1, 1, 125, 32000), dtype=float32)
array([[ -100,  -100,  -100, ..., 18514, 15084, 29892],
       [ -100,  -100,  -100, ..., 10983,  3245, 29936],
       [ -100,  -100,  -100, ...,  2266,   338,   263],
       ...,
       [ -100,  -100,  -100, ...,  1125,    13,  4706],
       [ -100,  -100,  -100, ..., 29900, 29901,    13],
       [ -100,  -100,  -100, ...,  5480,   727,   338]])
--KeyboardInterrupt--
KeyboardInterrupt: Interrupted by user
KeyboardInterrupt
path:  ../../../../../model/TinyLlama-1.1B-Chat-v0.6
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../../../../model/TinyLlama-1.1B-Chat-v0.6 and are newly initialized: ['model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading data...
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Token indices sequence length is longer than the specified maximum sequence length for this model (156 > 128). Running this sequence through the model will result in indexing errors
> /tmp/ipykernel_169503/4075727162.py(7)compute_metrics()
      5         import pdb; pdb.set_trace()
      6         #print(labels.shape)
----> 7         medusa_logits = logits[0,:, : ].contiguous()
      8
      9         medusa_labels = labels[...,  1:].contiguous()
pred = <transformers.trainer_utils.EvalPrediction object at 0x7f92d4304af0>
pred = <transformers.trainer_utils.EvalPrediction object at 0x7f92d4304af0>
<transformers.trainer_utils.EvalPrediction object at 0x7f92d4304af0>
array([], shape=(0, 1, 125, 32000), dtype=float32)
array([[ -100,  -100,  -100, ..., 18514, 15084, 29892],
       [ -100,  -100,  -100, ..., 10983,  3245, 29936],
       [ -100,  -100,  -100, ...,  2266,   338,   263],
       ...,
       [ -100,  -100,  -100, ...,  1125,    13,  4706],
       [ -100,  -100,  -100, ..., 29900, 29901,    13],
       [ -100,  -100,  -100, ...,  5480,   727,   338]])
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 2048, padding_idx=0)
    (layers): ModuleList(
      (0-21): 22 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=256, bias=False)
          (v_proj): Linear(in_features=2048, out_features=256, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
)
*** NameError: name 'inputs' is not defined
array([[ -100,  -100,  -100, ..., 18514, 15084, 29892],
       [ -100,  -100,  -100, ..., 10983,  3245, 29936],
       [ -100,  -100,  -100, ...,  2266,   338,   263],
       ...,
       [ -100,  -100,  -100, ...,  1125,    13,  4706],
       [ -100,  -100,  -100, ..., 29900, 29901,    13],
       [ -100,  -100,  -100, ...,  5480,   727,   338]])
--KeyboardInterrupt--
KeyboardInterrupt: Interrupted by user