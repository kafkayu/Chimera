
Token indices sequence length is longer than the specified maximum sequence length for this model (1267 > 1024). Running this sequence through the model will result in indexing errors
> /tmp/ipykernel_26659/3286967680.py(3)compute_metrics()
      1 def compute_metrics(pred):
      2         import pdb; pdb.set_trace()
----> 3         labels = pred.label_ids
      4         logits = pred.predictions
      5         if logits == None:
*** NameError: name 'logits' is not defined
array([], shape=(0, 2, 1023, 32000), dtype=float32)
0
--KeyboardInterrupt--
KeyboardInterrupt: Interrupted by user
/tmp/ipykernel_26659/3286967680.py:5: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if logits == None:
KeyboardInterrupt
Loading data...
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
> /tmp/ipykernel_26659/3398946696.py(3)compute_metrics()
      1 def compute_metrics(pred):
      2         import pdb; pdb.set_trace()
----> 3         labels = pred.label_ids
      4         logits = pred.predictions
      5         if len(logits) ==0 :
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../../../../model/TinyLlama-1.1B-Chat-v0.6 and are newly initialized: ['model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
path:  ../../../../../model/TinyLlama-1.1B-Chat-v0.6
Loading data...
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Token indices sequence length is longer than the specified maximum sequence length for this model (1267 > 1024). Running this sequence through the model will result in indexing errors
> /tmp/ipykernel_26659/3398946696.py(3)compute_metrics()
      1 def compute_metrics(pred):
      2         import pdb; pdb.set_trace()
----> 3         labels = pred.label_ids
      4         logits = pred.predictions
      5         if len(logits) ==0 :
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../../../../model/TinyLlama-1.1B-Chat-v0.6 and are newly initialized: ['model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
path:  ../../../../../model/TinyLlama-1.1B-Chat-v0.6
Loading data...
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Token indices sequence length is longer than the specified maximum sequence length for this model (1267 > 1024). Running this sequence through the model will result in indexing errors
> /tmp/ipykernel_26659/3398946696.py(3)compute_metrics()
      1 def compute_metrics(pred):
      2         import pdb; pdb.set_trace()
----> 3         labels = pred.label_ids
      4         logits = pred.predictions
      5         if len(logits) ==0 :
--- Logging error ---
Traceback (most recent call last):
  File "/var/conda/lib/python3.10/logging/__init__.py", line 1104, in emit
    self.flush()
  File "/var/conda/lib/python3.10/logging/__init__.py", line 1084, in flush
    self.stream.flush()
OSError: [Errno 5] Input/output error
Call stack:
  File "/var/conda/lib/python3.10/threading.py", line 973, in _bootstrap
    self._bootstrap_inner()
  File "/var/conda/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/var/conda/lib/python3.10/site-packages/ipykernel/parentpoller.py", line 38, in run
    get_logger().warning("Parent appears to have exited, shutting down.")
Message: 'Parent appears to have exited, shutting down.'
Arguments: ()