> /tmp/ipykernel_225059/3234715425.py(125)prediction_step()
    123                     loss = loss.mean().detach()
    124                     import pdb;pdb.set_trace()
--> 125                     if isinstance(outputs, dict):
    126                         logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + ["loss"])
    127                     else:
> /tmp/ipykernel_225059/3234715425.py(149)prediction_step()
    147             logits = logits[0]
    148         import pdb;pdb.set_trace()
--> 149         return (loss, logits, labels)
    150     def compute_loss(self, model, inputs, return_outputs=False):
    151         # DDP will give us model.module
/var/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 4, 1020, 32000])) that is different to the input size (torch.Size([4, 1020, 32000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
> /tmp/ipykernel_225059/3234715425.py(125)prediction_step()
    123                     loss = loss.mean().detach()
    124                     import pdb;pdb.set_trace()
--> 125                     if isinstance(outputs, dict):
    126                         logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + ["loss"])
    127                     else:
> /tmp/ipykernel_225059/3234715425.py(149)prediction_step()
    147             logits = logits[0]
    148         import pdb;pdb.set_trace()
--> 149         return (loss, logits, labels)
    150     def compute_loss(self, model, inputs, return_outputs=False):
    151         # DDP will give us model.module
tensor([[[[-21.2656, -21.9531,  -1.5957,  ...,  -8.6406, -17.3125,  -4.3594],
          [-24.5625, -24.2188,   1.5781,  ..., -10.2344, -21.6562,  -9.1328],
          [-30.2188, -30.5156,   1.1182,  ..., -12.5938, -24.3281, -11.6328],
          ...,
          [-27.2656, -28.1562,   3.1309,  ..., -19.9219, -21.2969, -21.7188],
          [-25.9688, -25.6875,   2.1016,  ..., -20.0312, -20.9688, -20.8906],
          [-25.5312, -26.1562,   3.0371,  ..., -21.1875, -19.6562, -21.0625]],
         [[-21.2656, -21.9531,  -1.5957,  ...,  -8.6406, -17.3125,  -4.3594],
          [-24.5625, -24.2188,   1.5781,  ..., -10.2344, -21.6562,  -9.1328],
          [-30.2188, -30.5156,   1.1182,  ..., -12.5938, -24.3281, -11.6328],
          ...,
          [-25.8438, -26.1406,   9.1484,  ..., -13.5547, -22.6094, -18.2812],
          [-25.4219, -25.8594,   8.4062,  ..., -13.3906, -22.2969, -17.8906],
          [-25.7188, -26.7188,   8.1641,  ..., -13.7812, -22.2656, -17.5781]],
         [[-21.2656, -21.9531,  -1.5957,  ...,  -8.6406, -17.3125,  -4.3594],
          [-24.5625, -24.2188,   1.5781,  ..., -10.2344, -21.6562,  -9.1328],
          [-30.2188, -30.5156,   1.1182,  ..., -12.5938, -24.3281, -11.6328],
          ...,
          [-24.7656, -25.6406,   3.3066,  ..., -14.3906, -16.7812, -17.6562],
          [-21.4688, -23.2812,   3.6543,  ..., -13.3516, -17.0781, -16.4219],
          [-24.3125, -25.0938,   2.4883,  ..., -13.9531, -16.3750, -22.1094]],
         [[-21.2656, -21.9531,  -1.5957,  ...,  -8.6406, -17.3125,  -4.3594],
          [-24.5625, -24.2188,   1.5781,  ..., -10.2344, -21.6562,  -9.1328],
          [-30.2188, -30.5156,   1.1182,  ..., -12.5938, -24.3281, -11.6328],
          ...,
          [-29.6406, -32.2500,   3.8809,  ..., -22.4062, -25.2031, -22.4688],
          [-29.7031, -32.0000,   5.7461,  ..., -19.5469, -23.2969, -21.6562],
          [-28.8906, -30.0156,   4.6875,  ..., -20.8750, -23.3281, -20.6719]]]],
       device='cuda:0')
> /tmp/ipykernel_225059/3234715425.py(125)prediction_step()
    123                     loss = loss.mean().detach()
    124                     import pdb;pdb.set_trace()
--> 125                     if isinstance(outputs, dict):
    126                         logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + ["loss"])
    127                     else:
> /tmp/ipykernel_225059/3234715425.py(149)prediction_step()
    147             logits = logits[0]
    148         import pdb;pdb.set_trace()
--> 149         return (loss, logits, labels)
    150     def compute_loss(self, model, inputs, return_outputs=False):
    151         # DDP will give us model.module
/var/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1, 1020, 32000])) that is different to the input size (torch.Size([1, 1020, 32000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
> /tmp/ipykernel_225059/3234715425.py(125)prediction_step()
    123                     loss = loss.mean().detach()
    124                     import pdb;pdb.set_trace()
--> 125                     if isinstance(outputs, dict):
    126                         logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + ["loss"])
    127                     else:
> /tmp/ipykernel_225059/3234715425.py(149)prediction_step()
    147             logits = logits[0]
    148         import pdb;pdb.set_trace()
--> 149         return (loss, logits, labels)
    150     def compute_loss(self, model, inputs, return_outputs=False):
    151         # DDP will give us model.module
> /tmp/ipykernel_225059/775524108.py(10)compute_metrics()
      8         import pdb; pdb.set_trace()
      9         #print(labels.shape)
---> 10         medusa_logits = logits[0,:, : ].contiguous()
     11
     12         medusa_labels = labels[...,  1:].contiguous()
*** AttributeError: 'EvalPrediction' object has no attribute 'preditions'
array([[[[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -29.734375 ,  -28.953125 ,    7.8945312, ...,  -19.296875 ,
           -29.40625  ,  -20.015625 ],
         [ -29.84375  ,  -29.09375  ,    8.0078125, ...,  -19.28125  ,
           -29.546875 ,  -20.40625  ],
         [ -29.734375 ,  -29.171875 ,    8.3671875, ...,  -19.46875  ,
           -29.8125   ,  -20.203125 ]],
        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -27.0625   ,  -27.40625  ,    6.875    , ...,  -17.890625 ,
           -23.515625 ,  -19.46875  ],
         [ -26.90625  ,  -27.609375 ,    6.3515625, ...,  -17.890625 ,
           -23.671875 ,  -19.96875  ],
         [ -26.90625  ,  -27.578125 ,    6.7382812, ...,  -17.953125 ,
           -22.625    ,  -19.75     ]],
        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -27.734375 ,  -30.359375 ,    3.1640625, ...,  -20.25     ,
           -20.484375 ,  -21.34375  ],
         [ -29.375    ,  -31.359375 ,    5.53125  , ...,  -20.       ,
           -22.84375  ,  -20.5625   ],
         [ -26.78125  ,  -29.1875   ,    2.6132812, ...,  -19.625    ,
           -21.515625 ,  -17.40625  ]],
        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -27.984375 ,  -26.84375  ,    4.890625 , ...,  -14.25     ,
           -21.171875 ,  -18.53125  ],
         [ -27.328125 ,  -25.46875  ,    6.6992188, ...,  -14.5234375,
           -20.078125 ,  -18.578125 ],
         [ -24.828125 ,  -24.234375 ,    2.9375   , ...,  -14.6953125,
           -17.796875 ,  -16.96875  ]]],
       [[[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -27.265625 ,  -28.15625  ,    3.1308594, ...,  -19.921875 ,
           -21.296875 ,  -21.71875  ],
         [ -25.96875  ,  -25.6875   ,    2.1015625, ...,  -20.03125  ,
           -20.96875  ,  -20.890625 ],
         [ -25.53125  ,  -26.15625  ,    3.0371094, ...,  -21.1875   ,
           -19.65625  ,  -21.0625   ]],
        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -25.84375  ,  -26.140625 ,    9.1484375, ...,  -13.5546875,
           -22.609375 ,  -18.28125  ],
         [ -25.421875 ,  -25.859375 ,    8.40625  , ...,  -13.390625 ,
           -22.296875 ,  -17.890625 ],
         [ -25.71875  ,  -26.71875  ,    8.1640625, ...,  -13.78125  ,
           -22.265625 ,  -17.578125 ]],
        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -24.765625 ,  -25.640625 ,    3.3066406, ...,  -14.390625 ,
           -16.78125  ,  -17.65625  ],
         [ -21.46875  ,  -23.28125  ,    3.6542969, ...,  -13.3515625,
           -17.078125 ,  -16.421875 ],
         [ -24.3125   ,  -25.09375  ,    2.4882812, ...,  -13.953125 ,
           -16.375    ,  -22.109375 ]],
        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -29.640625 ,  -32.25     ,    3.8808594, ...,  -22.40625  ,
           -25.203125 ,  -22.46875  ],
         [ -29.703125 ,  -32.       ,    5.7460938, ...,  -19.546875 ,
           -23.296875 ,  -21.65625  ],
         [ -28.890625 ,  -30.015625 ,    4.6875   , ...,  -20.875    ,
           -23.328125 ,  -20.671875 ]]],
       [[[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -29.234375 ,  -28.78125  ,    4.5078125, ...,  -17.203125 ,
           -19.375    ,  -18.65625  ],
         [ -29.296875 ,  -28.515625 ,    6.15625  , ...,  -17.03125  ,
           -21.765625 ,  -19.609375 ],
         [ -28.90625  ,  -28.484375 ,    6.       , ...,  -18.171875 ,
           -18.609375 ,  -18.140625 ]],
        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -29.703125 ,  -28.96875  ,    6.6445312, ...,  -21.84375  ,
           -23.671875 ,  -20.375    ],
         [ -29.84375  ,  -29.34375  ,    6.5195312, ...,  -21.765625 ,
           -23.28125  ,  -20.671875 ],
         [ -29.734375 ,  -28.59375  ,    6.4804688, ...,  -21.8125   ,
           -23.296875 ,  -20.859375 ]],
        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -34.6875   ,  -35.5625   ,    4.9726562, ...,  -24.21875  ,
           -27.546875 ,  -24.921875 ],
         [ -35.4375   ,  -36.03125  ,    6.0742188, ...,  -26.03125  ,
           -30.953125 ,  -26.828125 ],
         [ -32.0625   ,  -33.625    ,    5.2148438, ...,  -25.15625  ,
           -27.046875 ,  -23.078125 ]],
        [[ -21.265625 ,  -21.953125 ,   -1.5957031, ...,   -8.640625 ,
           -17.3125   ,   -4.359375 ],
         [ -24.5625   ,  -24.21875  ,    1.578125 , ...,  -10.234375 ,
           -21.65625  ,   -9.1328125],
         [ -30.21875  ,  -30.515625 ,    1.1181641, ...,  -12.59375  ,
           -24.328125 ,  -11.6328125],
         ...,
         [ -28.21875  ,  -27.75     ,    7.0585938, ...,  -13.359375 ,
           -25.484375 ,  -19.359375 ],
         [ -27.78125  ,  -26.875    ,    7.1484375, ...,  -13.5859375,
           -25.03125  ,  -18.859375 ],
         [ -27.90625  ,  -26.59375  ,    8.4296875, ...,  -13.2734375,
           -25.28125  ,  -18.453125 ]]],
       [[[ -21.3125   ,  -22.015625 ,   -1.6005859, ...,   -8.6796875,
           -17.359375 ,   -4.390625 ],
         [ -24.5625   ,  -24.234375 ,    1.5625   , ...,  -10.234375 ,
           -21.671875 ,   -9.140625 ],
         [ -30.265625 ,  -30.5625   ,    1.1201172, ...,  -12.625    ,
           -24.375    ,  -11.6640625],
         ...,
         [ -31.25     ,  -32.875    ,    5.6601562, ...,  -18.484375 ,
           -28.734375 ,  -20.203125 ],
         [ -27.015625 ,  -29.359375 ,    3.1484375, ...,  -18.28125  ,
           -25.625    ,  -18.640625 ],
         [ -29.59375  ,  -30.96875  ,    4.3085938, ...,  -18.       ,
           -27.8125   ,  -19.78125  ]],
        [[-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         ...,
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ]],
        [[-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         ...,
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ]],
        [[-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         ...,
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ],
         [-100.       , -100.       , -100.       , ..., -100.       ,
          -100.       , -100.       ]]]], dtype=float32)
(4, 4, 1020, 32000)
*** AttributeError: 'EvalPrediction' object has no attribute 'labels'
*** AttributeError: 'EvalPrediction' object has no attribute 'label'
array([[ -100,  -100,  -100, ...,  -100,  -100,  -100],
       [ -100,  -100,  -100, ...,  -100,  -100,  -100],
       [ -100,  -100,  -100, ...,   893, 29874,   313],
       ...,
       [ -100,  -100,  -100, ..., 29879,  1950,   393],
       [ -100,  -100,  -100, ...,  -100,  -100,  -100],
       [ -100,  -100,  -100, ...,   890,    13,    13]])
(13, 1024)
(4, 4, 1020, 32000)
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../../../../model/TinyLlama-1.1B-Chat-v0.6 and are newly initialized: ['model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
path:  ../../../../../model/TinyLlama-1.1B-Chat-v0.6
Loading data...
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
> /tmp/ipykernel_225059/3234715425.py(125)prediction_step()
    123                     loss = loss.mean().detach()
    124                     import pdb;pdb.set_trace()
--> 125                     if isinstance(outputs, dict):
    126                         logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + ["loss"])
    127                     else:
tensor(22.9332, device='cuda:0')
*** NameError: name 'logits' is not defined
> /tmp/ipykernel_225059/3234715425.py(149)prediction_step()
    147             logits = logits[0]
    148         import pdb;pdb.set_trace()
--> 149         return (loss, logits, labels)
    150     def compute_loss(self, model, inputs, return_outputs=False):
    151         # DDP will give us model.module
torch.Size([1, 4, 1020, 32000])
torch.Size([1, 4, 1020, 32000])
--KeyboardInterrupt--
KeyboardInterrupt: Interrupted by user
> /tmp/ipykernel_225059/3234715425.py(125)prediction_step()
    123                     loss = loss.mean().detach()
    124                     import pdb;pdb.set_trace()
--> 125                     if isinstance(outputs, dict):
    126                         logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + ["loss"])
