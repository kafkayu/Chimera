{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b9422f-cb7c-40ec-a4f3-143a0e0f6b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 20:53:59.684362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/var/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:106: UserWarning: \n",
      "\n",
      "================================================================================\n",
      "WARNING: Manual override via BNB_CUDA_VERSION env variable detected!\n",
      "BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "Loading CUDA version: BNB_CUDA_VERSION=122\n",
      "================================================================================\n",
      "\n",
      "\n",
      "  warn((f'\\n\\n{\"=\"*80}\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-23 20:54:01,706] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# This code is based on tatsu-lab/stanford_alpaca. Below is the original copyright:\n",
    "#\n",
    "#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "\n",
    "# Adapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer, BitsAndBytesConfig\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "\n",
    "from fastchat.conversation import SeparatorStyle\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "from torch.nn import CrossEntropyLoss,MSELoss\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "from medusa.model.medusa_model import MedusaModel, MedusaConfig,SingleMedusa\n",
    "import torch.nn.functional as F\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "\n",
    "\n",
    "# Customized for training Medusa heads\n",
    "class CustomizedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # DDP will give us model.module\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if hasattr(model, \"module\"):\n",
    "            medusa = model.module.medusa\n",
    "        else:\n",
    "            medusa = model.medusa\n",
    "\n",
    "        logits1 = model(\n",
    "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        logits =logits1['logtis']\n",
    "        \n",
    "        labels = inputs[\"labels\"]\n",
    "        # Shift so that tokens < n predict n\n",
    "        loss = 0\n",
    "        loss_fct =CrossEntropyLoss()\n",
    "        log = {}\n",
    "        #logits = torch.clamp(logits, min=1e-7, max=100 - 1e-7)\n",
    "        for i in range(medusa):\n",
    "            #########修改后#######\n",
    "            # medusa_logits = logits[i, :, : -1].contiguous()\n",
    "            \n",
    "            # medusa_labels = labels[...,  2:].contiguous()\n",
    "            ######原medusa#########\n",
    "            \n",
    "            medusa_logits = logits[i, :, : ].contiguous()\n",
    "            \n",
    "            medusa_labels = labels[...,  3:].contiguous()\n",
    "            medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "            medusa_labels = medusa_labels.view(-1)\n",
    "            \n",
    "            medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "            \n",
    "            #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=100 - 1e-7)\n",
    "           \n",
    "            loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "            loss += loss_i\n",
    "            not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "            medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "            # Add top-k accuracy\n",
    "            for k in range(1, 6):\n",
    "                _, topk = medusa_logits.topk(k, dim=-1)\n",
    "                topk = topk[not_ignore]\n",
    "                correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "                log[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "        \n",
    "            \n",
    "            log[f\"medusa{i}_loss\"] = loss_i.item()\n",
    "            #log[f\"medusa{i}_loss_7\"] = loss_i_7.item()\n",
    "        self.log(log)\n",
    "        if model.training == False :\n",
    "            logeval  = {}\n",
    "            for k in range(1, 6):\n",
    "                logeval[f\"eval_medusa{0}_top{k}\"] = log[f\"medusa{0}_top{k}\"]\n",
    "            logeval[f\"eval_medusa{0}_loss\"] = log[f\"medusa{0}_loss\"]\n",
    "            self.log(logeval)\n",
    "        return (loss, logits) if return_outputs else loss\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"lmsys/vicuna-7b-v1.3\")\n",
    "    load_in_4bit: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Load in 4 bit.\"},\n",
    "    )\n",
    "    load_in_8bit: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Load in 8 bit.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(\n",
    "        default=\"sharegpt_clean.json\",\n",
    "        metadata={\"help\": \"Path to the training data.\"},\n",
    "    )\n",
    "    eval_data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n",
    "    )\n",
    "    lazy_preprocess: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=2048,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    medusa_num_heads: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Number of Medusa heads.\"},\n",
    "    )\n",
    "    medusa_num_layers: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Number of layers for each Medusa head.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "local_rank = None\n",
    "\n",
    "\n",
    "def rank0_print(*args):\n",
    "    if local_rank == 0:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conv = get_conversation_template(\"vicuna\")\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}, {j}, {role}, {conv.roles[j % 2]}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "    input_ids = tokenizer(\n",
    "        conversations,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    targets = input_ids.clone()\n",
    "\n",
    "    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n",
    "\n",
    "    # Mask targets. Only compute loss on the assistant outputs.\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        turns = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_TOKEN_ID\n",
    "        for i, turn in enumerate(turns):\n",
    "            if turn == \"\":\n",
    "                break\n",
    "            turn_len = len(tokenizer(turn).input_ids)\n",
    "\n",
    "            parts = turn.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            # \"-2\" is hardcoded for the LLaMA tokenizer to make the offset correct.\n",
    "            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            # Ignore the user instructions\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n",
    "            cur_len += turn_len\n",
    "\n",
    "        target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "        if False:  # Inspect and check the correctness of masking\n",
    "            z = target.clone()\n",
    "            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
    "            rank0_print(tokenizer.decode(z))\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_TOKEN_ID\n",
    "                rank0_print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "                    f\" (ignored)\"\n",
    "                )\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        rank0_print(\"Formatting inputs...\")\n",
    "        sources = [example[\"conversations\"] for example in raw_data]\n",
    "        data_dict = preprocess(sources, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )\n",
    "\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.raw_data = raw_data\n",
    "        self.cached_data_dict = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        if i in self.cached_data_dict:\n",
    "            return self.cached_data_dict[i]\n",
    "\n",
    "        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer)\n",
    "        ret = dict(\n",
    "            input_ids=ret[\"input_ids\"][0],\n",
    "            labels=ret[\"labels\"][0],\n",
    "            attention_mask=ret[\"attention_mask\"][0],\n",
    "        )\n",
    "        self.cached_data_dict[i] = ret\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "def make_supervised_data_module(\n",
    "    tokenizer: transformers.PreTrainedTokenizer, data_args\n",
    ") -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    dataset_cls = (\n",
    "        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n",
    "    )\n",
    "    rank0_print(\"Loading data...\")\n",
    "\n",
    "    train_json = json.load(open(data_args.data_path, \"r\"))\n",
    "    train_dataset = dataset_cls(train_json, tokenizer=tokenizer)\n",
    "\n",
    "    if data_args.eval_data_path:\n",
    "        eval_json = json.load(open(data_args.eval_data_path, \"r\"))\n",
    "        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer)\n",
    "    else:\n",
    "        eval_dataset = None\n",
    "\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6f7e0a-5077-4a29-80b3-47bbdbe8d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "global local_rank\n",
    "import transformers \n",
    "#from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    local_rank=0,\n",
    "    model_max_length=128 ,\n",
    "    medusa_num_heads = 1 ,\n",
    "    medusa_num_layers =  1 ,\n",
    "    prediction_loss_only = False,\n",
    "    output_dir= './test', \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 5 ,\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=1e-3, \n",
    "    weight_decay=0.0,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    #fp16=True, #对应--bf16\n",
    "    #tf32=True,\n",
    "    \n",
    ")\n",
    "#from transformers import DataArguments\n",
    "\n",
    "# data_args = DataArguments(\n",
    "#     data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "#     eval_data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "#     lazy_preprocess= True \n",
    "# )\n",
    "#from transformers import ModelArguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    \n",
    "    model_name_or_path=\"../../../../../model/TinyLlama-1.1B-Chat-v0.6\",\n",
    "    #model_max_length=2048,\n",
    "    #lazy_preprocess=True,\n",
    "    # medusa_num_heads=3,\n",
    "    # medusa_num_layers=1\n",
    ")\n",
    "data_args = DataArguments(\n",
    "    data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "    eval_data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "    lazy_preprocess= True \n",
    ")\n",
    "local_rank =0 # training_args.local_rank\n",
    "\n",
    "# Set RoPE scaling factor\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    ")\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len and training_args.model_max_length > orig_ctx_len:\n",
    "    scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n",
    "    config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "config.use_cache = False\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052cc18c-8a98-4e40-b244-4aaf088b2ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0749e20-9e00-469c-87ff-cdb9a81a204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "        logits,labels = pred\n",
    "        #logits = pred.predictions\n",
    "        #print(logits.shape)\n",
    "        import pdb; pdb.set_trace()\n",
    "        #print(labels.shape)\n",
    "        medusa_logits = logits[0,:, : ].contiguous()\n",
    "            \n",
    "        medusa_labels = labels[...,  1:].contiguous()\n",
    "        medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "        medusa_labels = medusa_labels.view(-1)\n",
    "        \n",
    "        medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "        \n",
    "        #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=1 - 1e-7)\n",
    "       \n",
    "        loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "        loss += loss_i\n",
    "        not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "        medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "        # Add top-k accuracy\n",
    "        for k in range(1, 6):\n",
    "            _, topk = medusa_logits.topk(k, dim=-1)\n",
    "            topk = topk[not_ignore]\n",
    "            correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "            log[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "            #res[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "    \n",
    "        \n",
    "        log[f\"medusa{i}_loss\"] = loss_i.item()\n",
    "\n",
    "        \n",
    "        return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575dc631-f426-4937-bed7-75afe3bd110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../../../../model/TinyLlama-1.1B-Chat-v0.6 and are newly initialized: ['model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ../../../../../model/TinyLlama-1.1B-Chat-v0.6\n",
      "Loading data...\n",
      "Formatting inputs...Skip in lazy mode\n",
      "Formatting inputs...Skip in lazy mode\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global local_rank\n",
    "\n",
    "\n",
    "local_rank =0 #training_args.local_rank\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    ")\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len and training_args.model_max_length > orig_ctx_len:\n",
    "    scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n",
    "    config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "config.use_cache = False\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config if model_args.load_in_4bit else None,\n",
    "    load_in_4bit=model_args.load_in_4bit,\n",
    "    load_in_8bit=model_args.load_in_8bit,\n",
    ")\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "medusa_lm_head = MedusaModel(\n",
    "    model,\n",
    "    medusa_num_heads=training_args.medusa_num_heads,\n",
    "    medusa_num_layers=training_args.medusa_num_layers,\n",
    "    base_model_name_or_path=model_args.model_name_or_path\n",
    ")\n",
    "for param in medusa_lm_head.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "training_args.output_dir = f\"{training_args.output_dir}_medusa_mlp_{model_args.model_name_or_path.split('/')[-1]}_medusa_{training_args.medusa_num_heads}_lr_{training_args.learning_rate}_layers_{training_args.medusa_num_layers}\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# Load data\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "\n",
    "# Generate Medusa config for pushing to HF hub\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "# Start trainner\n",
    "trainer = CustomizedTrainer(\n",
    "    model=medusa_lm_head, tokenizer=tokenizer, args=training_args, **data_module\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd603355-2f5e-4aa9-abc8-23095fb8a0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86572ea4-efb7-449e-aca3-159d5a0d6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(medusa_lm_head.fast_layer)\n",
    "for  i in medusa_lm_head.fast_layer.parameters() :\n",
    "   i.require_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2872b87-552a-411e-b41f-dfd1e0da2bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7debb2ee-d552-4878-abcc-15a1619a2470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myu13668962105\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/liyunhao/nlptest/medusa/Medusa/medusa/train/wandb/run-20231215_213514-h4ot268l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yu13668962105/huggingface/runs/h4ot268l' target=\"_blank\">./test</a></strong> to <a href='https://wandb.ai/yu13668962105/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yu13668962105/huggingface' target=\"_blank\">https://wandb.ai/yu13668962105/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yu13668962105/huggingface/runs/h4ot268l' target=\"_blank\">https://wandb.ai/yu13668962105/huggingface/runs/h4ot268l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>118.500000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>52.250000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13, training_loss=101.66346153846153, metrics={'train_runtime': 14.2175, 'train_samples_per_second': 0.914, 'train_steps_per_second': 0.914, 'total_flos': 0.0, 'train_loss': 101.66346153846153, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae7c2c8-e1cc-4e4b-b3b8-7368bc072f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5215e1f-022a-4ff1-9f53-59dcffecffdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
