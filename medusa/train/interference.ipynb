{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d1c74fb4-c3f2-4575-9597-b9a00472faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer, BitsAndBytesConfig\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "\n",
    "from fastchat.conversation import SeparatorStyle\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from medusa.model.medusa_model import MedusaModel, MedusaConfig,SingleMedusa\n",
    "import torch.nn.functional as F\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ef1e8e54-b088-4571-93d3-385479090f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timed(wall_times, key):\n",
    "    start = time.time()\n",
    "    torch.cuda.synchronize()\n",
    "    yield\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    wall_times[key].append(elapsed_time)\n",
    "\n",
    "def medusa_forward(input_ids, model, tokenizer, medusa_buffers, medusa_topk, temperature, posterior_threshold, posterior_alpha, past_key_values, past_key_values_data, current_length_data, steps = 512):\n",
    "    wall_times = {'medusa': [], 'tree': [], 'posterior': [], 'update': [], 'init': []}\n",
    "    \n",
    "    with timed(wall_times, 'init'):\n",
    "        reset_medusa_mode(model)\n",
    "        input_len = input_ids.shape[1]\n",
    "        medusa_logits, logits = initialize_medusa(input_ids, model, medusa_buffers['medusa_attn_mask'], past_key_values)\n",
    "    \n",
    "    new_token = 0\n",
    "\n",
    "    for idx in range(steps): \n",
    "        with timed(wall_times, 'medusa'):\n",
    "            candidates, tree_candidates = generate_candidates(medusa_logits, logits, medusa_topk, medusa_buffers['tree_indices'], temperature)\n",
    "\n",
    "        with timed(wall_times, 'tree'):\n",
    "            medusa_logits, logits, outputs = tree_decoding(model, tree_candidates, past_key_values, medusa_buffers['medusa_position_ids'], input_ids, medusa_buffers['retrieve_indices'])\n",
    "\n",
    "        with timed(wall_times, 'posterior'):\n",
    "            best_candidate, accept_length = evaluate_posterior(logits, candidates, temperature, posterior_threshold, posterior_alpha)\n",
    "        \n",
    "        with timed(wall_times, 'update'):\n",
    "            input_ids, logits, medusa_logits, new_token = update_inference_inputs(input_ids, candidates, best_candidate, accept_length, medusa_buffers['retrieve_indices'], outputs, logits, medusa_logits, new_token, past_key_values_data, current_length_data)\n",
    "\n",
    "        if tokenizer.eos_token_id in input_ids[0, input_len:].tolist():\n",
    "            break\n",
    "\n",
    "    return input_ids, new_token, idx, wall_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d454660b-33ba-4a49-adb4-1363dc20a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_name = '../../../../idea8_t1_medusa_mlp_TinyLlama-1.1B-Chat-v0.6_medusa_1_lr_0.0005_layers_1/checkpoint-7200/pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "853029aa-ef72-454d-9500-99ac31ffa154",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict =torch.load(state_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "df212b09-5ab2-406b-9c13-45fc26e63d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"../../../../../model/TinyLlama-1.1B-Chat-v0.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "22ca24f1-e131-4747-a3ae-2df3f8ed09a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5b64a2f5-6d6d-493d-95b1-5097b1d3c356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../../../../model/TinyLlama-1.1B-Chat-v0.6 and are newly initialized: ['model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a21e7189-ea9b-4e43-bcea-fbe90b494318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ../../../../../model/TinyLlama-1.1B-Chat-v0.6\n"
     ]
    }
   ],
   "source": [
    "medusa_lm_head = MedusaModel(\n",
    "        model,\n",
    "        medusa_num_heads=1,\n",
    "        medusa_num_layers=1,\n",
    "        base_model_name_or_path=model_name_or_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a8d1ffaa-bb18-4914-ae4f-659b88395462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medusa_lm_head.load_state_dict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a95708bd-8077-4979-8d6c-4287eea8c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_max_length=2048\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    model_max_length=model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "77364449-3c7b-487e-a3c3-82ae68138612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(fastmodel,input_ids,attention_mask,outputs,k=5):\n",
    "        embed =fastmodel.base_model.model.embed_tokens(input_ids)\n",
    "        embedtrigram = torch.cat((embed[:,:-2],embed[:,1:-1],embed[:,2:]),dim=-1)\n",
    "        gram1 = torch.cat((embed[:,0],embed[:,1],embed[:,1]),dim=-1).unsqueeze(1)\n",
    "        embedtrigram = torch.cat((gram1,embedtrigram),dim=-2)\n",
    "        embed = fastmodel.trimlp(embedtrigram )\n",
    "        from modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n",
    "        batch_size, seq_length = embed.shape[:2]\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(\n",
    "                         attention_mask[:,:-1], (batch_size, seq_length), embed, 0\n",
    "                    )\n",
    "        attention_mask  = attention_mask.to(fastmodel.base_model.device) \n",
    "        input1 = torch.cat((outputs[0],embed[:,:]),dim=-1)\n",
    "        output_fastlayer =    fastmodel.fast_layer1(input1,attention_mask = attention_mask)\n",
    "        output_fastlayer = fastmodel.fastoutput(output_fastlayer[0])\n",
    "        logits = fastmodel.medusa_head[0](output_fastlayer)\n",
    "        candidate = logits[0][-1].topk(k=5)[1]\n",
    "        return logits,candidate\n",
    "def calacc(input,max_length = 100,k=3):\n",
    "    input = tokenizer([inputs])\n",
    "    input_ids = torch.tensor(input.input_ids)\n",
    "    attention_mask = torch.tensor(input.attention_mask)\n",
    "    count = 0\n",
    "    for i in range(max_length):\n",
    "        outputs = medusa_lm_head.base_model.model(input_ids ,attention_mask = attention_mask ,output_hidden_states=True)\n",
    "        orig =  medusa_lm_head.base_model.lm_head(outputs[0])\n",
    "        t0 = torch.argmax(orig[0][-1])\n",
    "        input_ids  = torch.cat((input_ids,t0.unsqueeze(0).unsqueeze(0)),dim=-1)\n",
    "        attention_mask = torch.cat((attention_mask,torch.tensor([[1]])),dim=-1)\n",
    "        \n",
    "        l,ca = generate(medusa_lm_head,input_ids,attention_mask,outputs,k=k)\n",
    "        realt1 =torch.argmax( medusa_lm_head.base_model(input_ids = input_ids)[0][0][-1])\n",
    "        count+=sum(ca.eq(realt1))\n",
    "    print(count/100)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "10815d33-3737-460b-a152-cfea5eca11bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"who are you?Assistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ee0f7d9e-ca28-4611-bca3-db1473675a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9800)\n"
     ]
    }
   ],
   "source": [
    "output = calacc(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "306b44b3-eade-4a5e-9376-e70b9ed86873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>who are you?Assistant: I am a person who is always looking for new experiences and learning new things. I am curious about the world around me and I love to explore new places and meet new people. I am always looking for ways to make a positive impact on the world around me. I am passionate about helping others and making a difference in their lives. I am always looking for ways to make a positive impact on the world around me. I am always looking for ways to make a positive impact on the world around'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbcf11-d583-43d7-b5d7-e4503b8d6726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
