{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a602fc2b-cebf-4ad4-b46a-252cbd1d4e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19410712-5f58-467e-9943-0534bef0b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is based on tatsu-lab/stanford_alpaca. Below is the original copyright:\n",
    "#\n",
    "from transformers.trainer import *\n",
    "#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "\n",
    "# Adapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer, BitsAndBytesConfig\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "\n",
    "from fastchat.conversation import SeparatorStyle\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from medusa.model.medusa_model import MedusaModel, MedusaConfig,SingleMedusa\n",
    "import torch.nn.functional as F\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "\n",
    "\n",
    "# Customized for training Medusa heads\n",
    "class CustomizedTrainer(Trainer):\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on `model` using `inputs`.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "            ignore_keys (`List[str]`, *optional*):\n",
    "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
    "                gathering predictions.\n",
    "\n",
    "        Return:\n",
    "            Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
    "            logits and labels (each being optional).\n",
    "        \"\"\"\n",
    "        has_labels = False if len(self.label_names) == 0 else all(inputs.get(k) is not None for k in self.label_names)\n",
    "        # For CLIP-like models capable of returning loss values.\n",
    "        # If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`\n",
    "        # is `True` in `model.forward`.\n",
    "        return_loss = inputs.get(\"return_loss\", None)\n",
    "        if return_loss is None:\n",
    "            return_loss = self.can_return_loss\n",
    "        loss_without_labels = True if len(self.label_names) == 0 and return_loss else False\n",
    "\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
    "        if has_labels or loss_without_labels:\n",
    "            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
    "            if len(labels) == 1:\n",
    "                labels = labels[0]\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if is_sagemaker_mp_enabled():\n",
    "                raw_outputs = smp_forward_only(model, inputs)\n",
    "                if has_labels or loss_without_labels:\n",
    "                    if isinstance(raw_outputs, dict):\n",
    "                        loss_mb = raw_outputs[\"loss\"]\n",
    "                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "                    else:\n",
    "                        loss_mb = raw_outputs[0]\n",
    "                        logits_mb = raw_outputs[1:]\n",
    "\n",
    "                    loss = loss_mb.reduce_mean().detach().cpu()\n",
    "                    logits = smp_nested_concat(logits_mb)\n",
    "                else:\n",
    "                    loss = None\n",
    "                    if isinstance(raw_outputs, dict):\n",
    "                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys)\n",
    "                    else:\n",
    "                        logits_mb = raw_outputs\n",
    "                    logits = smp_nested_concat(logits_mb)\n",
    "            else:\n",
    "                if has_labels or loss_without_labels:\n",
    "                    with self.compute_loss_context_manager():\n",
    "                        loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "                    loss = loss.mean().detach()\n",
    "\n",
    "                    if isinstance(outputs, dict):\n",
    "                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "                    else:\n",
    "                        logits = outputs[:][0]\n",
    "                else:\n",
    "                    loss = None\n",
    "                    with self.compute_loss_context_manager():\n",
    "                        outputs = model(**inputs)\n",
    "\n",
    "                    if isinstance(outputs, dict):\n",
    "                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n",
    "                    else:\n",
    "                        logits = outputs\n",
    "                    # TODO: this needs to be fixed and made cleaner later.\n",
    "                    if self.args.past_index >= 0:\n",
    "                        self._past = outputs[self.args.past_index - 1]\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        logits = nested_detach(logits)\n",
    "        if len(logits) == 1:\n",
    "            logits = logits[0]\n",
    "\n",
    "        return (loss, logits, labels)\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # DDP will give us model.module\n",
    "        if hasattr(model, \"module\"):\n",
    "            medusa = model.module.medusa\n",
    "        else:\n",
    "            medusa = model.medusa\n",
    "\n",
    "        logits = model(\n",
    "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        logits =logits['logits']\n",
    "        \n",
    "        labels = inputs[\"labels\"]\n",
    "        # Shift so that tokens < n predict n\n",
    "        loss = 0\n",
    "        loss_fct =CrossEntropyLoss()\n",
    "        log = {}\n",
    "        #logits = torch.clamp(logits, min=1e-7, max=100 - 1e-7)\n",
    "        for i in range(medusa):\n",
    "            #########修改后#######\n",
    "            # medusa_logits = logits[i, :, : -1].contiguous()\n",
    "            \n",
    "            # medusa_labels = labels[...,  2:].contiguous()\n",
    "            ######原medusa#########\n",
    "            \n",
    "            medusa_logits = logits[i, :, : ].contiguous()\n",
    "            \n",
    "            medusa_labels = labels[...,  3:].contiguous()\n",
    "            medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "            medusa_labels = medusa_labels.view(-1)\n",
    "            \n",
    "            medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "            \n",
    "            #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=100 - 1e-7)\n",
    "\n",
    "            loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "            loss += loss_i\n",
    "            not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "            medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "            # Add top-k accuracy\n",
    "            for k in range(1, 6):\n",
    "                _, topk = medusa_logits.topk(k, dim=-1)\n",
    "                topk = topk[not_ignore]\n",
    "                correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "                log[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "        \n",
    "            \n",
    "            log[f\"medusa{i}_loss\"] = loss_i.item()\n",
    "            #log[f\"medusa{i}_loss_7\"] = loss_i_7.item()\n",
    "        self.log(log)\n",
    "        return (loss, logits) if return_outputs else loss\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        # output_dir = self.args.output_dir\n",
    "        # 创建输出目录\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    " \n",
    "        # 保存训练参数\n",
    "        torch.save(\n",
    "        self.model.trimlp.state_dict(),\n",
    "        os.path.join(output_dir, \"medusa_lm_head.pt\"),\n",
    "    )\n",
    "        torch.save(self.model.fast_layer1.state_dict(), os.path.join(output_dir, \"fast_layer1.pt\"))\n",
    " \n",
    "        # # 保存有梯度变化的模型参数\n",
    "        # saved_params = {\n",
    "        #     k: v.to(\"cpu\") for k, v in self.model.named_parameters() if v.requires_grad\n",
    "        # }\n",
    "        torch.save(self.model.medusa_head.state_dict(), os.path.join(output_dir, \"medusa_head.pt\"))\n",
    "    def compute_metrics(pred):\n",
    "        logits,labels = pred\n",
    "        loss = 0\n",
    "        log = {}\n",
    "        lables = pred.label_ids\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        #logits = pred.predictions\n",
    "        #print(logits.shape)\n",
    "        logits = torch.tensor(logits)\n",
    "        labels = torch.tensor(labels)\n",
    "        import pdb; pdb.set_trace()\n",
    "        #print(labels.shape)\n",
    "        medusa_logits = logits.contiguous()\n",
    "            \n",
    "        medusa_labels = labels[...,4:].contiguous()\n",
    "        medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "        medusa_labels = medusa_labels.view(-1)\n",
    "        \n",
    "        medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "        import pdb;pdb.set_trace()\n",
    "        #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=1 - 1e-7)\n",
    "        \n",
    "        loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "        loss += loss_i\n",
    "        not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "        medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "        # Add top-k accuracy\n",
    "        for k in range(1, 6):\n",
    "            _, topk = medusa_logits.topk(k, dim=-1)\n",
    "            topk = topk[not_ignore]\n",
    "            correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "            log[f\"medusa{0}_top{k}\"] = correct.float().mean().item()\n",
    "            #res[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "    \n",
    "        \n",
    "        log[f\"medusa{0}_loss\"] = loss_i.item()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return log\n",
    "\n",
    " \n",
    "    # def compute_metrics(pred):\n",
    "    #     labels,logits = pred.label_ids\n",
    "    #     logits = pred.predictions\n",
    "    #     medusa_logits = logits[i, :, : -1].contiguous()\n",
    "            \n",
    "    #     medusa_labels = labels[...,  1:].contiguous()\n",
    "    #     medusa_logits = medusa_logits.view(-1, logits.shape[-1])\n",
    "    #     medusa_labels = medusa_labels.view(-1)\n",
    "        \n",
    "    #     medusa_labels = medusa_labels.to(medusa_logits.device)\n",
    "        \n",
    "    #     #medusa_logits = torch.clamp(medusa_logits, min=1e-7, max=1 - 1e-7)\n",
    "       \n",
    "    #     loss_i = loss_fct(medusa_logits, medusa_labels)\n",
    "    #     loss += loss_i\n",
    "    #     not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)\n",
    "    #     medusa_labels = medusa_labels[not_ignore]\n",
    "\n",
    "    #     # Add top-k accuracy\n",
    "    #     for k in range(1, 6):\n",
    "    #         _, topk = medusa_logits.topk(k, dim=-1)\n",
    "    #         topk = topk[not_ignore]\n",
    "    #         correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)\n",
    "    #         log[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "    #         res[f\"medusa{i}_top{k}\"] = correct.float().mean().item()\n",
    "    \n",
    "        \n",
    "    #     log[f\"medusa{i}_loss\"] = loss_i.item()\n",
    "\n",
    "        \n",
    "    #     return log\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"lmsys/vicuna-7b-v1.3\")\n",
    "    load_in_4bit: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Load in 4 bit.\"},\n",
    "    )\n",
    "    load_in_8bit: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Load in 8 bit.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(\n",
    "        default=\"sharegpt_clean.json\",\n",
    "        metadata={\"help\": \"Path to the training data.\"},\n",
    "    )\n",
    "    eval_data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n",
    "    )\n",
    "    lazy_preprocess: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=2048,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    medusa_num_heads: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Number of Medusa heads.\"},\n",
    "    )\n",
    "    medusa_num_layers: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Number of layers for each Medusa head.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "local_rank = None\n",
    "\n",
    "\n",
    "def rank0_print(*args):\n",
    "    if local_rank == 0:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conv = get_conversation_template(\"vicuna\")\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}, {j}, {role}, {conv.roles[j % 2]}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "    input_ids = tokenizer(\n",
    "        conversations,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    targets = input_ids.clone()\n",
    "\n",
    "    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n",
    "\n",
    "    # Mask targets. Only compute loss on the assistant outputs.\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        turns = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_TOKEN_ID\n",
    "        for i, turn in enumerate(turns):\n",
    "            if turn == \"\":\n",
    "                break\n",
    "            turn_len = len(tokenizer(turn).input_ids)\n",
    "\n",
    "            parts = turn.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            # \"-2\" is hardcoded for the LLaMA tokenizer to make the offset correct.\n",
    "            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            # Ignore the user instructions\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n",
    "            cur_len += turn_len\n",
    "\n",
    "        target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "        if False:  # Inspect and check the correctness of masking\n",
    "            z = target.clone()\n",
    "            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
    "            rank0_print(tokenizer.decode(z))\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_TOKEN_ID\n",
    "                rank0_print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "                    f\" (ignored)\"\n",
    "                )\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        rank0_print(\"Formatting inputs...\")\n",
    "        sources = [example[\"conversations\"] for example in raw_data]\n",
    "        data_dict = preprocess(sources, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )\n",
    "\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.raw_data = raw_data\n",
    "        self.cached_data_dict = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        if i in self.cached_data_dict:\n",
    "            return self.cached_data_dict[i]\n",
    "\n",
    "        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer)\n",
    "        ret = dict(\n",
    "            input_ids=ret[\"input_ids\"][0],\n",
    "            labels=ret[\"labels\"][0],\n",
    "            attention_mask=ret[\"attention_mask\"][0],\n",
    "        )\n",
    "        self.cached_data_dict[i] = ret\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "def make_supervised_data_module(\n",
    "    tokenizer: transformers.PreTrainedTokenizer, data_args\n",
    ") -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    dataset_cls = (\n",
    "        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n",
    "    )\n",
    "    rank0_print(\"Loading data...\")\n",
    "\n",
    "    train_json = json.load(open(data_args.data_path, \"r\"))\n",
    "    train_dataset = dataset_cls(train_json, tokenizer=tokenizer)\n",
    "\n",
    "    if data_args.eval_data_path:\n",
    "        eval_json = json.load(open(data_args.eval_data_path, \"r\"))\n",
    "        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer)\n",
    "    else:\n",
    "        eval_dataset = None\n",
    "\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "016c268d-0a2f-49f7-b739-793a424c258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "global local_rank\n",
    "import transformers \n",
    "#from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    local_rank=0,\n",
    "    model_max_length=1024 ,\n",
    "    medusa_num_heads = 1 ,\n",
    "    medusa_num_layers =  1 ,\n",
    "    output_dir= './test', \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 1 ,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps = 2 ,\n",
    "    save_total_limit = 2,\n",
    "    learning_rate=1e-3, \n",
    "    weight_decay=0.0,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    fp16=True, #对应--bf16\n",
    "    tf32=True,\n",
    "    \n",
    ")\n",
    "#from transformers import DataArguments\n",
    "\n",
    "data_args = DataArguments(\n",
    "    data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "    eval_data_path=\"../../../../../data/ShareGPT_Vicuna_unfiltered/small_test.json\",\n",
    "    lazy_preprocess= True \n",
    ")\n",
    "#from transformers import ModelArguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    \n",
    "    model_name_or_path=\"../../../../../model/TinyLlama-1.1B-Chat-v0.6\",\n",
    "    #model_max_length=2048,\n",
    "    #lazy_preprocess=True,\n",
    "    # medusa_num_heads=3,\n",
    "    # medusa_num_layers=1\n",
    ")\n",
    "\n",
    "local_rank =0 # training_args.local_rank\n",
    "\n",
    "# Set RoPE scaling factor\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    ")\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len and training_args.model_max_length > orig_ctx_len:\n",
    "    scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n",
    "    config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "config.use_cache = False\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7baa1fb8-2f65-46f1-a870-69009ae5b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ca29f9e-ead5-4cc3-b95f-5222149b07ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../../../../model/TinyLlama-1.1B-Chat-v0.6 and are newly initialized: ['model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        low_cpu_mem_usage=True,\n",
    "        #torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config if model_args.load_in_4bit else None,\n",
    "        load_in_4bit=model_args.load_in_4bit,\n",
    "        load_in_8bit=model_args.load_in_8bit,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8603169c-d3b5-45ca-a71a-885125d51c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ../../../../../model/TinyLlama-1.1B-Chat-v0.6\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "#model_name = '../../../../idea6_3fastlayer_t1_skipbert_medusa_mlp_vicuna-7b-v1.3_medusa_1_lr_0.0001_layers_1'\n",
    "#medusa_lm_head = MedusaModel.from\n",
    "# for param in medusa_lm_head.base_model.parameters():\n",
    "#         param.requires_grad = False\n",
    "medusa_lm_head = MedusaModel(\n",
    "        model,\n",
    "        medusa_num_heads=training_args.medusa_num_heads,\n",
    "        medusa_num_layers=training_args.medusa_num_layers,\n",
    "        base_model_name_or_path=model_args.model_name_or_path\n",
    "    )\n",
    "training_args.output_dir = f\"{training_args.output_dir}_medusa_mlp_{model_args.model_name_or_path.split('/')[-1]}_medusa_{training_args.medusa_num_heads}_lr_{training_args.learning_rate}_layers_{training_args.medusa_num_layers}\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b3dd854-df65-4832-b8a2-e82cd0c97e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Formatting inputs...Skip in lazy mode\n",
      "Formatting inputs...Skip in lazy mode\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "#compute metrics\n",
    "\n",
    "# Generate Medusa config for pushing to HF hub\n",
    "medusa_config = MedusaConfig(\n",
    "    medusa_num_heads=training_args.medusa_num_heads,\n",
    "    medusa_num_layers=training_args.medusa_num_layers,\n",
    "    base_model_name_or_path=model_args.model_name_or_path,\n",
    ")\n",
    "\n",
    "# Save Medusa config\n",
    "medusa_config.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "# Start trainner\n",
    "trainer = CustomizedTrainer(\n",
    "    model=medusa_lm_head, tokenizer=tokenizer, args=training_args , **data_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "791531e1-3a1c-4555-bef8-21aeed36e6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9/13 00:22 < 00:13, 0.31 it/s, Epoch 0.62/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>22.634900</td>\n",
       "      <td>22.112837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>21.242400</td>\n",
       "      <td>22.112837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>21.850800</td>\n",
       "      <td>22.112837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>22.062800</td>\n",
       "      <td>22.112837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>24.895000</td>\n",
       "      <td>22.112837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>21.522100</td>\n",
       "      <td>27.046387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>25.780000</td>\n",
       "      <td>27.046387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>29.162000</td>\n",
       "      <td>27.046387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1, 1020, 32000])) that is different to the input size (torch.Size([1, 1020, 32000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/var/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1, 1020, 32000])) that is different to the input size (torch.Size([1, 1020, 32000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/var/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1, 1020, 32000])) that is different to the input size (torch.Size([1, 1020, 32000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ab5c824-8c1b-40a6-8a63-b07b179437c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_name = './test_medusa_mlp_TinyLlama-1.1B-Chat-v0.6_medusa_1_lr_0.001_layers_1/checkpoint-4/fast_layer1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f781c74e-5bc9-4265-bc10-82b93fac8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict =torch.load(state_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2316a06d-575f-49e1-ba5d-197100cb816d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.fast_layer1.load_state_dict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963759ae-8b26-4c5a-9b97-40fb10f8378d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
