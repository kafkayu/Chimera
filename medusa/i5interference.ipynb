{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0350356c-9a94-474a-a40b-3a44548af51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:106: UserWarning: \n",
      "\n",
      "================================================================================\n",
      "WARNING: Manual override via BNB_CUDA_VERSION env variable detected!\n",
      "BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "Loading CUDA version: BNB_CUDA_VERSION=122\n",
      "================================================================================\n",
      "\n",
      "\n",
      "  warn((f'\\n\\n{\"=\"*80}\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-24 22:09:59,387] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 22:10:00.894111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" # define GPU id, remove if you want to use all GPUs available\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "from medusa.model.modeling_llama_kv import LlamaForCausalLM as KVLlamaForCausalLM\n",
    "from medusa.model.medusa_model import MedusaModel\n",
    "from medusa.model.kv_cache import *\n",
    "from medusa.model.utils import *\n",
    "#from medusa.model.medusa_choices import *\n",
    "import transformers\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ebf945-7afc-44b7-b436-d0e11fd14ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" # define GPU id, remove if you want to use all GPUs available\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "from medusa.model.modeling_llama_kv import LlamaForCausalLM as KVLlamaForCausalLM\n",
    "from medusa.model.medusa_model import MedusaModel\n",
    "from medusa.model.kv_cache import *\n",
    "from medusa.model.utils import *\n",
    "#from medusa.model.medusa_choices import *\n",
    "import transformers\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2c525f-4e15-4bf8-9d8d-a3f98abd0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '../../../../idea5_3gram_4fastlayer_t1_skipbert_teacherstudent_2_medusa_mlp_vicuna-7b-v1.3_medusa_1_lr_0.0001_layers_1/checkpoint-1200/pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7705bb71-5c6f-4eb9-80ff-77e14e2cc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2beeffbc-1f4f-4bab-866c-ec666d21b96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff62e03092cd4c5e8e298ef94048c386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path ../../../../../model/vicuna-7b-v1.3\n",
      "path:  ../../../../../model/vicuna-7b-v1.3\n"
     ]
    }
   ],
   "source": [
    "model_name2 = '../../../../idea5_3gram_4fastlayer_t1_skipbert_teacherstudent_2_medusa_mlp_vicuna-7b-v1.3_medusa_1_lr_0.0001_layers_1'\n",
    "model2 = MedusaModel.from_pretrained(\n",
    "    model_name2,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1dfea8a-bc41-449d-99ad-a6e50b3396bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ce643e1-fea2-4410-be92-4b8ada41aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model2.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c036694b-e705-4455-8775-35c7bae6d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.\n",
    "posterior_threshold = 0.09\n",
    "posterior_alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb671375-eecf-43cc-8f6b-88dee47ac209",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Hi, could you share a tale about a charming llama that grows Medusa-like hair and starts its own coffee shop? ASSISTANT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28624a30-845b-41dc-a2c5-7d268440db8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'base_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m num_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(medusa_choices) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m medusa_topk \u001b[38;5;241m=\u001b[39m medusa_choices[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m----> 7\u001b[0m medusa_buffers \u001b[38;5;241m=\u001b[39m generate_medusa_buffers(medusa_choices, device\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'base_model'"
     ]
    }
   ],
   "source": [
    "tokenizer = model2.get_tokenizer()\n",
    "\n",
    "medusa_choices = torch.tensor([2,5])#([5,5,5,5,5])#4,5\n",
    "num_heads = len(medusa_choices) - 1\n",
    "medusa_topk = medusa_choices[1:]\n",
    "\n",
    "medusa_buffers = generate_medusa_buffers(medusa_choices, device=model.base_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82215ee5-275d-480f-bbe6-4fd0f7849129",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values, past_key_values_data, current_length_data = initialize_past_key_values(model.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dfa837-27a7-46ca-a1a0-8a0d25f9565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_buffers,\n",
    "                    medusa_topk,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                    past_key_values,\n",
    "                    past_key_values_data, current_length_data\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b73ca6-be00-4d4d-a040-645d344506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c066137-2ff8-4e97-a414-f0ea32baf0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
