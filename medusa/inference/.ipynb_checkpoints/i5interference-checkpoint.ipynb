{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0350356c-9a94-474a-a40b-3a44548af51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:106: UserWarning: \n",
      "\n",
      "================================================================================\n",
      "WARNING: Manual override via BNB_CUDA_VERSION env variable detected!\n",
      "BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "Loading CUDA version: BNB_CUDA_VERSION=122\n",
      "================================================================================\n",
      "\n",
      "\n",
      "  warn((f'\\n\\n{\"=\"*80}\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-25 00:20:17,087] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 00:20:18.618270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" # define GPU id, remove if you want to use all GPUs available\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "from medusa.model.modeling_llama_kv import LlamaForCausalLM as KVLlamaForCausalLM\n",
    "from medusa.model.medusa_model import MedusaModel\n",
    "from medusa.model.kv_cache import *\n",
    "from medusa.model.utils import *\n",
    "#from medusa.model.medusa_choices import *\n",
    "import transformers\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b378b658-fe8a-4694-86f6-34acefdeabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_medusa(input_ids, model, medusa_attn_mask, past_key_values,attention_mask):\n",
    "    \"\"\"\n",
    "    Initializes the Medusa structure for a given model.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    1. Forward pass through the model to obtain the Medusa logits, original model outputs, and logits.\n",
    "    2. Sets the Medusa attention mask within the base model.\n",
    "\n",
    "    Args:\n",
    "    - input_ids (torch.Tensor): The input tensor containing token ids.\n",
    "    - model (MedusaLMHead): The model containing the Medusa layers and base model.\n",
    "    - medusa_attn_mask (torch.Tensor): The attention mask designed specifically for the Medusa structure.\n",
    "    - past_key_values (list of torch.Tensor): Contains past hidden states and past attention values.\n",
    "\n",
    "    Returns:\n",
    "    - medusa_logits (torch.Tensor): Logits from the Medusa heads.\n",
    "    - logits (torch.Tensor): Original logits from the base model.\n",
    "    \"\"\"\n",
    "    print(attention_mask)\n",
    "    medusa_logits, outputs, logits = model(\n",
    "        input_ids, attention_mask = attention_mask , output_orig=True# past_key_values=past_key_values\n",
    "    )\n",
    "    model.base_model.model.medusa_mask = medusa_attn_mask\n",
    "    return medusa_logits, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ebf945-7afc-44b7-b436-d0e11fd14ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timed(wall_times, key):\n",
    "    start = time.time()\n",
    "    torch.cuda.synchronize()\n",
    "    yield\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    wall_times[key].append(elapsed_time)\n",
    "\n",
    "def medusa_forward(input_ids, model, tokenizer, medusa_buffers, medusa_topk, temperature, posterior_threshold, posterior_alpha, past_key_values, past_key_values_data, current_length_data, attention_mask ,steps = 512):\n",
    "    wall_times = {'medusa': [], 'tree': [], 'posterior': [], 'update': [], 'init': []}\n",
    "    print(attention_mask)\n",
    "    with timed(wall_times, 'init'):\n",
    "        reset_medusa_mode(model)\n",
    "        input_len = input_ids.shape[1]\n",
    "        medusa_logits, logits = initialize_medusa(input_ids, model, medusa_buffers['medusa_attn_mask'], past_key_values,attention_mask)\n",
    "    \n",
    "    new_token = 0\n",
    "\n",
    "    for idx in range(steps): \n",
    "        with timed(wall_times, 'medusa'):\n",
    "            candidates, tree_candidates = generate_candidates(medusa_logits, logits, medusa_topk, medusa_buffers['tree_indices'], temperature)\n",
    "\n",
    "        with timed(wall_times, 'tree'):\n",
    "            medusa_logits, logits, outputs = tree_decoding(model, tree_candidates, past_key_values, medusa_buffers['medusa_position_ids'], input_ids, medusa_buffers['retrieve_indices'])\n",
    "\n",
    "        with timed(wall_times, 'posterior'):\n",
    "            best_candidate, accept_length = evaluate_posterior(logits, candidates, temperature, posterior_threshold, posterior_alpha)\n",
    "        \n",
    "        with timed(wall_times, 'update'):\n",
    "            input_ids, logits, medusa_logits, new_token = update_inference_inputs(input_ids, candidates, best_candidate, accept_length, medusa_buffers['retrieve_indices'], outputs, logits, medusa_logits, new_token, past_key_values_data, current_length_data)\n",
    "\n",
    "        if tokenizer.eos_token_id in input_ids[0, input_len:].tolist():\n",
    "            break\n",
    "\n",
    "    return input_ids, new_token, idx, wall_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2c525f-4e15-4bf8-9d8d-a3f98abd0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '../../../../idea5_3gram_4fastlayer_t1_skipbert_teacherstudent_2_medusa_mlp_vicuna-7b-v1.3_medusa_1_lr_0.0001_layers_1/checkpoint-1800/pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7705bb71-5c6f-4eb9-80ff-77e14e2cc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2beeffbc-1f4f-4bab-866c-ec666d21b96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d7ce2bce7d46f8a6ada52d700a0282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path ../../../../../model/vicuna-7b-v1.3\n",
      "path:  ../../../../../model/vicuna-7b-v1.3\n"
     ]
    }
   ],
   "source": [
    "model_name2 = '../../../../idea5_3gram_4fastlayer_t1_skipbert_teacherstudent_2_medusa_mlp_vicuna-7b-v1.3_medusa_1_lr_0.0001_layers_1'\n",
    "model2 = MedusaModel.from_pretrained(\n",
    "    model_name2,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1dfea8a-bc41-449d-99ad-a6e50b3396bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce643e1-fea2-4410-be92-4b8ada41aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model2.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c036694b-e705-4455-8775-35c7bae6d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.\n",
    "posterior_threshold = 0.09\n",
    "posterior_alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb671375-eecf-43cc-8f6b-88dee47ac209",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Hi, could you share a tale about a charming llama that grows Medusa-like hair and starts its own coffee shop? ASSISTANT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fb39e3f-8599-4320-acc2-d0c8e6d6833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28624a30-845b-41dc-a2c5-7d268440db8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyunhao/nlptest/medusa/Medusa/medusa/model/utils.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  medusa_choices = torch.tensor(medusa_choices)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = model2.get_tokenizer()\n",
    "\n",
    "medusa_choices = torch.tensor([2,5])#([5,5,5,5,5])#4,5\n",
    "num_heads = len(medusa_choices) - 1\n",
    "medusa_topk = medusa_choices[1:]\n",
    "\n",
    "medusa_buffers = generate_medusa_buffers(medusa_choices, device=model2.base_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd65dbc8-d892-4142-aadc-106621d3dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        past_key_values=None,\n",
    "        output_orig=False,\n",
    "        position_ids=None,\n",
    "        \n",
    "    ):\n",
    "        \"\"\"Forward pass of the MedusaModel.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor, optional): Input token IDs.\n",
    "            attention_mask (torch.Tensor, optional): Attention mask.\n",
    "            labels (torch.Tensor, optional): Ground truth labels for loss computation.\n",
    "            past_key_values (tuple, optional): Tuple containing past key and value states for attention.\n",
    "            output_orig (bool, optional): Whether to also output predictions from the original LM head.\n",
    "            position_ids (torch.Tensor, optional): Position IDs.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing predictions from all Medusa heads.\n",
    "            (Optional) Original predictions from the base model's LM head.\n",
    "        \"\"\"\n",
    "        with torch.inference_mode():\n",
    "            # Pass input through the base model\n",
    "            outputs = self.base_model.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                past_key_values=past_key_values,\n",
    "                position_ids=position_ids,\n",
    "                #output_hidden_states=True,\n",
    "            )\n",
    "            if output_orig:\n",
    "                orig = self.base_model.lm_head(outputs[0])\n",
    "        \n",
    "        #####1.获取fastlayer层#####\n",
    "        embed =self.base_model.model.embed_tokens(input_ids)\n",
    "        embedtrigram = torch.cat((embed[:,:-2],embed[:,1:-1],embed[:,2:],),dim=-1)\n",
    "        embed = self.trimlp(embedtrigram )\n",
    "        from modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n",
    "        batch_size, seq_length = embed.shape[:2]\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(\n",
    "                         attention_mask[:,:-2], (batch_size, seq_length), embed, 0\n",
    "                    )\n",
    "        attention_mask  = attention_mask.to(self.base_model.device)\n",
    "        # embedtrigram = torch.cat((embed[:,:-2],embed[:,1:-1],embed[:,2:]),dim=-1)\n",
    "        #for  i in self.fast_layer :      \n",
    "        embed = self.fast_layer1(embed ,attention_mask = attention_mask)\n",
    "        embed = self.fast_layer2(embed[0] ,attention_mask = attention_mask)\n",
    "        embed = self.fast_layer3(embed[0] ,attention_mask = attention_mask)\n",
    "        embed = self.fast_layer4(embed[0] ,attention_mask = attention_mask)\n",
    "        embed = embed[0]\n",
    "        loss_fct = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "        hsloss =loss_fct( outputs[0][:,2:].clone(),embed[:,:])\n",
    "        embed3 = torch.cat((outputs[0][:,1:-1],embed[:,:]),dim=-1)#output2[0][:,-seq_length+2:-1]\n",
    "        medusa_logits = []\n",
    "        # TODO: Consider parallelizing this loop for efficiency?\n",
    "        for i in range(self.medusa):\n",
    "            #######修改后输出######\n",
    "            medusa_logits.append(self.medusa_head[i](embed3.unsqueeze(0)))#self.medusa_head[i]embed3.unsqueeze(0)(outputs[0]))#hidden_states[i*4].clone()))gruout.to(self.base_model.dtype)\n",
    "            ######原输出######\n",
    "            #medusa_logits.append(self.medusa_head[i]((outputs[0].clone())))\n",
    "        # if output_orig:\n",
    "        #     return torch.stack(medusa_logits, dim=0), outputs, orig\n",
    "        if output_orig:\n",
    "            return torch.stack(medusa_logits, dim=0), outputs, orig\n",
    "        return {\"logits\":torch.stack(medusa_logits, dim=0),\"hsloss\":hsloss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7b15725-3701-4910-8cf4-a44560cd4382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 263, 2793, 2998,  278, 4249], device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor([ 1339, 26373, 24600, 18708,  9914], device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor([ 8238,   347, 18708,   368,   993], device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor([5697,  322,  282, 4509, 1766], device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor([  310,   491,  4249, 29892,   322], device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor([ 278, 1749,  902,  263,  365], device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor([4870, 3942, 7881, 6350, 3815], device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor([ 3942,  7881,  2291,  6289, 29889], device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor([ 4870,    13,  2296,  2318, 29889], device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor([ 8126,   471,   750, 10398,   727], device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    for i in range(10):\n",
    "        input = tokenizer([prompt])\n",
    "        input_ids = torch.as_tensor(input.input_ids).cuda()\n",
    "        attention_mask =torch.as_tensor(input.attention_mask).cuda()\n",
    "        output = single_forward(model,input_ids,attention_mask=attention_mask,output_orig = True)\n",
    "        token1 = torch.argmax(output[-1][-1][-1])\n",
    "        \n",
    "        input_ids = torch.cat((input_ids,token1.unsqueeze(0).unsqueeze(0)),dim=-1)\n",
    "        prompt = tokenizer.decode(\n",
    "                        input_ids[0],\n",
    "                        spaces_between_special_tokens=False,\n",
    "                    )\n",
    "        input = tokenizer([prompt])\n",
    "        input_ids = torch.as_tensor(input.input_ids).cuda()\n",
    "        attention_mask =torch.as_tensor(input.attention_mask).cuda()\n",
    "        output1 = model(input_ids,attention_mask=attention_mask,output_orig = True)\n",
    "        token_ref = torch.argmax(output1[-1][-1][-1])\n",
    "        token2 = torch.topk(output[0][-1][-1][-1][-1],5)[1]\n",
    "        # print( token_ref)\n",
    "        print(token2)\n",
    "        correct = token2.eq(token_ref.unsqueeze(-1)).any(-1)\n",
    "        print(correct)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7afe83f-1524-4dc5-aecf-addb42756d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29902, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(output[0][0][0][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ac205ba-6f67-4e62-b7ea-35c26773d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = torch.argmax(output[1][0][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df06e097-e2f8-40e4-9030-33fcdc9071b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(output[1][0][0][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82215ee5-275d-480f-bbe6-4fd0f7849129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_key_values, past_key_values_data, current_length_data = initialize_past_key_values(model.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18dfa837-27a7-46ca-a1a0-8a0d25f9565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode():\n",
    "#     input = tokenizer([prompt])\n",
    "#     input_ids = input.input_ids\n",
    "#     attention_mask =torch.as_tensor(input.attention_mask).cuda()\n",
    "#     print(attention_mask)\n",
    "#     output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "#                     torch.as_tensor(input_ids).cuda(),\n",
    "#                     model,\n",
    "#                     tokenizer,\n",
    "#                     medusa_buffers,\n",
    "#                     medusa_topk,\n",
    "#                     temperature,\n",
    "#                     posterior_threshold,\n",
    "#                     posterior_alpha,\n",
    "#                     past_key_values,\n",
    "#                     past_key_values_data,\n",
    "#                     current_length_data,\n",
    "#                     attention_mask = attention_mask  ,\n",
    "#                 )\n",
    "#     output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "#     print(\"Output length:\", output_ids.size(-1))\n",
    "#     print(\"Compression ratio:\", new_token / idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b73ca6-be00-4d4d-a040-645d344506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c066137-2ff8-4e97-a414-f0ea32baf0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
