{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0350356c-9a94-474a-a40b-3a44548af51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:106: UserWarning: \n",
      "\n",
      "================================================================================\n",
      "WARNING: Manual override via BNB_CUDA_VERSION env variable detected!\n",
      "BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "Loading CUDA version: BNB_CUDA_VERSION=122\n",
      "================================================================================\n",
      "\n",
      "\n",
      "  warn((f'\\n\\n{\"=\"*80}\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-25 00:20:17,087] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 00:20:18.618270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" # define GPU id, remove if you want to use all GPUs available\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "from medusa.model.modeling_llama_kv import LlamaForCausalLM as KVLlamaForCausalLM\n",
    "from medusa.model.medusa_model import MedusaModel\n",
    "from medusa.model.kv_cache import *\n",
    "from medusa.model.utils import *\n",
    "#from medusa.model.medusa_choices import *\n",
    "import transformers\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b378b658-fe8a-4694-86f6-34acefdeabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_medusa(input_ids, model, medusa_attn_mask, past_key_values,attention_mask):\n",
    "    \"\"\"\n",
    "    Initializes the Medusa structure for a given model.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    1. Forward pass through the model to obtain the Medusa logits, original model outputs, and logits.\n",
    "    2. Sets the Medusa attention mask within the base model.\n",
    "\n",
    "    Args:\n",
    "    - input_ids (torch.Tensor): The input tensor containing token ids.\n",
    "    - model (MedusaLMHead): The model containing the Medusa layers and base model.\n",
    "    - medusa_attn_mask (torch.Tensor): The attention mask designed specifically for the Medusa structure.\n",
    "    - past_key_values (list of torch.Tensor): Contains past hidden states and past attention values.\n",
    "\n",
    "    Returns:\n",
    "    - medusa_logits (torch.Tensor): Logits from the Medusa heads.\n",
    "    - logits (torch.Tensor): Original logits from the base model.\n",
    "    \"\"\"\n",
    "    print(attention_mask)\n",
    "    medusa_logits, outputs, logits = model(\n",
    "        input_ids, attention_mask = attention_mask , output_orig=True# past_key_values=past_key_values\n",
    "    )\n",
    "    model.base_model.model.medusa_mask = medusa_attn_mask\n",
    "    return medusa_logits, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ebf945-7afc-44b7-b436-d0e11fd14ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timed(wall_times, key):\n",
    "    start = time.time()\n",
    "    torch.cuda.synchronize()\n",
    "    yield\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    wall_times[key].append(elapsed_time)\n",
    "\n",
    "def medusa_forward(input_ids, model, tokenizer, medusa_buffers, medusa_topk, temperature, posterior_threshold, posterior_alpha, past_key_values, past_key_values_data, current_length_data, attention_mask ,steps = 512):\n",
    "    wall_times = {'medusa': [], 'tree': [], 'posterior': [], 'update': [], 'init': []}\n",
    "    print(attention_mask)\n",
    "    with timed(wall_times, 'init'):\n",
    "        reset_medusa_mode(model)\n",
    "        input_len = input_ids.shape[1]\n",
    "        medusa_logits, logits = initialize_medusa(input_ids, model, medusa_buffers['medusa_attn_mask'], past_key_values,attention_mask)\n",
    "    \n",
    "    new_token = 0\n",
    "\n",
    "    for idx in range(steps): \n",
    "        with timed(wall_times, 'medusa'):\n",
    "            candidates, tree_candidates = generate_candidates(medusa_logits, logits, medusa_topk, medusa_buffers['tree_indices'], temperature)\n",
    "\n",
    "        with timed(wall_times, 'tree'):\n",
    "            medusa_logits, logits, outputs = tree_decoding(model, tree_candidates, past_key_values, medusa_buffers['medusa_position_ids'], input_ids, medusa_buffers['retrieve_indices'])\n",
    "\n",
    "        with timed(wall_times, 'posterior'):\n",
    "            best_candidate, accept_length = evaluate_posterior(logits, candidates, temperature, posterior_threshold, posterior_alpha)\n",
    "        \n",
    "        with timed(wall_times, 'update'):\n",
    "            input_ids, logits, medusa_logits, new_token = update_inference_inputs(input_ids, candidates, best_candidate, accept_length, medusa_buffers['retrieve_indices'], outputs, logits, medusa_logits, new_token, past_key_values_data, current_length_data)\n",
    "\n",
    "        if tokenizer.eos_token_id in input_ids[0, input_len:].tolist():\n",
    "            break\n",
    "\n",
    "    return input_ids, new_token, idx, wall_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2c525f-4e15-4bf8-9d8d-a3f98abd0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '../../../../idea5_3gram_4fastlayer_t1_skipbert_teacherstudent_2_medusa_mlp_vicuna-7b-v1.3_medusa_1_lr_0.0001_layers_1/checkpoint-1800/pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7705bb71-5c6f-4eb9-80ff-77e14e2cc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2beeffbc-1f4f-4bab-866c-ec666d21b96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d7ce2bce7d46f8a6ada52d700a0282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path ../../../../../model/vicuna-7b-v1.3\n",
      "path:  ../../../../../model/vicuna-7b-v1.3\n"
     ]
    }
   ],
   "source": [
    "model_name2 = '../../../../idea5_3gram_4fastlayer_t1_skipbert_teacherstudent_2_medusa_mlp_vicuna-7b-v1.3_medusa_1_lr_0.0001_layers_1'\n",
    "model2 = MedusaModel.from_pretrained(\n",
    "    model_name2,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1dfea8a-bc41-449d-99ad-a6e50b3396bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce643e1-fea2-4410-be92-4b8ada41aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model2.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c036694b-e705-4455-8775-35c7bae6d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.\n",
    "posterior_threshold = 0.09\n",
    "posterior_alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb671375-eecf-43cc-8f6b-88dee47ac209",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Hi, could you share a tale about a charming llama that grows Medusa-like hair and starts its own coffee shop? ASSISTANT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fb39e3f-8599-4320-acc2-d0c8e6d6833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28624a30-845b-41dc-a2c5-7d268440db8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyunhao/nlptest/medusa/Medusa/medusa/model/utils.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  medusa_choices = torch.tensor(medusa_choices)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = model2.get_tokenizer()\n",
    "\n",
    "medusa_choices = torch.tensor([2,5])#([5,5,5,5,5])#4,5\n",
    "num_heads = len(medusa_choices) - 1\n",
    "medusa_topk = medusa_choices[1:]\n",
    "\n",
    "medusa_buffers = generate_medusa_buffers(medusa_choices, device=model2.base_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd65dbc8-d892-4142-aadc-106621d3dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_forward1(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        past_key_values=None,\n",
    "        output_orig=False,\n",
    "        position_ids=None,\n",
    "        \n",
    "    ):\n",
    "        \"\"\"Forward pass of the MedusaModel.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor, optional): Input token IDs.\n",
    "            attention_mask (torch.Tensor, optional): Attention mask.\n",
    "            labels (torch.Tensor, optional): Ground truth labels for loss computation.\n",
    "            past_key_values (tuple, optional): Tuple containing past key and value states for attention.\n",
    "            output_orig (bool, optional): Whether to also output predictions from the original LM head.\n",
    "            position_ids (torch.Tensor, optional): Position IDs.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing predictions from all Medusa heads.\n",
    "            (Optional) Original predictions from the base model's LM head.\n",
    "        \"\"\"\n",
    "        with torch.inference_mode():\n",
    "            # Pass input through the base model\n",
    "            outputs = self.base_model.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                past_key_values=past_key_values,\n",
    "                position_ids=position_ids,\n",
    "                #output_hidden_states=True,\n",
    "            )\n",
    "            if output_orig:\n",
    "                orig = self.base_model.lm_head(outputs[0])\n",
    "        \n",
    "        #####1.获取fastlayer层#####\n",
    "        embed =self.base_model.model.embed_tokens(input_ids)\n",
    "        embedtrigram = torch.cat((embed[:,:-2],embed[:,1:-1],embed[:,2:],),dim=-1)\n",
    "        embed = self.trimlp(embedtrigram )\n",
    "        from modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n",
    "        batch_size, seq_length = embed.shape[:2]\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(\n",
    "                         attention_mask[:,:-2], (batch_size, seq_length), embed, 0\n",
    "                    )\n",
    "        attention_mask  = attention_mask.to(self.base_model.device)\n",
    "        # embedtrigram = torch.cat((embed[:,:-2],embed[:,1:-1],embed[:,2:]),dim=-1)\n",
    "        #for  i in self.fast_layer :      \n",
    "        embed = self.fast_layer1(embed ,attention_mask = attention_mask)\n",
    "        embed = self.fast_layer2(embed[0] ,attention_mask = attention_mask)\n",
    "        embed = self.fast_layer3(embed[0] ,attention_mask = attention_mask)\n",
    "        embed = self.fast_layer4(embed[0] ,attention_mask = attention_mask)\n",
    "        embed = embed[0]\n",
    "        loss_fct = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "        hsloss =loss_fct( outputs[0][:,2:].clone(),embed[:,:])\n",
    "        embed3 = torch.cat((outputs[0][:,1:-1],embed[:,:]),dim=-1)#output2[0][:,-seq_length+2:-1]\n",
    "        medusa_logits = []\n",
    "        # TODO: Consider parallelizing this loop for efficiency?\n",
    "        for i in range(self.medusa):\n",
    "            #######修改后输出######\n",
    "            medusa_logits.append(self.medusa_head[i](embed3.unsqueeze(0)))#self.medusa_head[i]embed3.unsqueeze(0)(outputs[0]))#hidden_states[i*4].clone()))gruout.to(self.base_model.dtype)\n",
    "            ######原输出######\n",
    "            #medusa_logits.append(self.medusa_head[i]((outputs[0].clone())))\n",
    "        # if output_orig:\n",
    "        #     return torch.stack(medusa_logits, dim=0), outputs, orig\n",
    "        if output_orig:\n",
    "            return torch.stack(medusa_logits, dim=0), outputs, orig\n",
    "        return {\"logits\":torch.stack(medusa_logits, dim=0),\"hsloss\":hsloss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6df6f575-8b09-4540-93a1-03d4b473eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \" USER: Hi, could you share a tale about a charming llama that grows Medusa-like hair and starts its own coffee shop? ASSISTANT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b6fd7aa9-51df-4fc7-9408-8b841b937da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.eq(token1k[0]).any(-1) or token2.eq(token1k[1]).any(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f7b15725-3701-4910-8cf4-a44560cd4382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856\n",
      "<s>  USER: Hi, could you share a tale about a charming llama that grows Medusa-like hair and starts its own coffee shop? ASSISTANT: Once upon a time, in a small village nestled in the Andes mountains, there lived a charming llama named Luna. Luna was known for her kind heart and her love of coffee. She would often spend her afternoons sipping on a steaming cup of joe at the local café, chatting with the villagers about their day.\n",
      "\n",
      "One day, as Luna was grazing in the fields, she noticed that her hair was starting to grow longer and thicker. At first, she didn't think much of it, but as the days went on, her hair grew longer and longer, until it was as long as a horse's mane.\n",
      "\n",
      "Luna was shocked by her transformation and didn't know what to do. But then, she remembered her love of coffee and decided to start her own café. She gathered all of her savings and bought a small cart, which she filled with the finest coffee beans from the village.\n",
      "\n",
      "Luna's Llama Café quickly became the talk of the town. People came from far and wide to try Luna's delicious coffee and to marvel at her Medusa-like hair. Luna was overjoyed by the success of her café and spent her days serving coffee and making new friends.\n",
      "\n",
      "As the years went on, Luna's hair continued to grow, but she never let it slow her down. She continued to run her café with grace and style, and her hair became a symbol of her strength and determination.\n",
      "\n",
      "And so, Luna's Llama Café became a beloved institution in the village, and Luna became a legend. She proved that with hard work and a little bit of magic, anything is possible.</s><s> #100DaysOfDiversity: Day 57 - The Importance of Representation in Media\n",
      "Today's topic is the importance of representation in media. Representation in media refers to the portrayal of different groups of people in media, such as television shows, movies, books, and magazines.\n",
      "Representation in media is important because it shapes how people see themselves and others. When people see themselves represented in media, it helps them feel seen and heard. It also helps to break down stereotypes and promote understanding and acceptance of different\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    count = 0\n",
    "    len=500\n",
    "    input = tokenizer([prompt])\n",
    "    input_ids = torch.as_tensor(input.input_ids).cuda()\n",
    "    attention_mask =torch.as_tensor(input.attention_mask).cuda()\n",
    "    for i in range(len):\n",
    "        \n",
    "        output = single_forward1(model,input_ids,attention_mask=attention_mask,output_orig = True)\n",
    "        token1 = torch.argmax(output[-1][-1][-1])\n",
    "        token1k =  torch.topk(output[-1][-1][-1],2)[1]\n",
    "        input_ids = torch.cat((input_ids,token1.unsqueeze(0).unsqueeze(0)),dim=-1)\n",
    "        attention_mask = torch.cat(( attention_mask,torch.tensor([1]).to(\"cuda\").unsqueeze(0)),dim=-1)\n",
    "        # prompt = tokenizer.decode(\n",
    "        #                 input_ids[0],\n",
    "        #                 #add_special_tokens=False,\n",
    "        #                 #spaces_between_special_tokens=False,\n",
    "        #             )\n",
    "        \n",
    "        # input_ids = torch.as_tensor(input.input_ids).cuda()\n",
    "        # attention_mask =torch.as_tensor(input.attention_mask).cuda()\n",
    "        # output1 = model(input_ids,attention_mask=attention_mask,output_orig = True)\n",
    "        # token_ref = torch.argmax(output1[-1][-1][-1])\n",
    "        token2 = torch.topk(output[0][-1][-1][-1][-1],5)[1]\n",
    "        # print( token_ref)\n",
    "        #print(token2)\n",
    "        correct = token2.eq(token1).any(-1) or token2.eq(token1k[1]).any(-1)\n",
    "        if correct==True :\n",
    "            count+=1\n",
    "    print( count/len)\n",
    "    output = tokenizer.decode(input_ids[0])   \n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f83422d0-6135-4ab0-bb29-1ec66cb594f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "03dec445-c6ca-478f-b39d-01370eb888d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you write me a homily on a particular subject?:'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea007da-f7a2-4994-9624-52d4cd9c7311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82215ee5-275d-480f-bbe6-4fd0f7849129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_key_values, past_key_values_data, current_length_data = initialize_past_key_values(model.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18dfa837-27a7-46ca-a1a0-8a0d25f9565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode():\n",
    "#     input = tokenizer([prompt])\n",
    "#     input_ids = input.input_ids\n",
    "#     attention_mask =torch.as_tensor(input.attention_mask).cuda()\n",
    "#     print(attention_mask)\n",
    "#     output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "#                     torch.as_tensor(input_ids).cuda(),\n",
    "#                     model,\n",
    "#                     tokenizer,\n",
    "#                     medusa_buffers,\n",
    "#                     medusa_topk,\n",
    "#                     temperature,\n",
    "#                     posterior_threshold,\n",
    "#                     posterior_alpha,\n",
    "#                     past_key_values,\n",
    "#                     past_key_values_data,\n",
    "#                     current_length_data,\n",
    "#                     attention_mask = attention_mask  ,\n",
    "#                 )\n",
    "#     output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "#     print(\"Output length:\", output_ids.size(-1))\n",
    "#     print(\"Compression ratio:\", new_token / idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b73ca6-be00-4d4d-a040-645d344506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c066137-2ff8-4e97-a414-f0ea32baf0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
